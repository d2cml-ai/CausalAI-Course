{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf0f1e7",
   "metadata": {},
   "source": [
    "# Inference on Predictive and Causal Effects in High-Dimensional Nonlinear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195cfe48",
   "metadata": {},
   "source": [
    "## Impact of 401(k) on Financial Wealth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1028f",
   "metadata": {},
   "source": [
    "As a practical illustration of the methods developed in this lecture, we consider estimation of the effect of 401(k) eligibility and participation on accumulated assets. 401(k) plans are pension accounts sponsored by employers. The key problem in determining the effect of participation in 401(k) plans on accumulated assets is saver heterogeneity coupled with the fact that the decision to enroll in a 401(k) is non-random. It is generally recognized that some people have a higher preference for saving than others. It also seems likely that those individuals with high unobserved preference for saving would be most likely to choose to participate in tax-advantaged retirement savings plans and would tend to have otherwise high amounts of accumulated assets. The presence of unobserved savings preferences with these properties then implies that conventional estimates that do not account for saver heterogeneity and endogeneity of participation will be biased upward, tending to overstate the savings effects of 401(k) participation.\n",
    "\n",
    "One can argue that eligibility for enrolling in a 401(k) plan in this data can be taken as exogenous after conditioning on a few observables of which the most important for their argument is income. The basic idea is that, at least around the time 401(k)’s initially became available, people were unlikely to be basing their employment decisions on whether an employer offered a 401(k) but would instead focus on income and other aspects of the job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a74326",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb5a12a",
   "metadata": {},
   "source": [
    "The data set comes from the `hdm` R package, and is loaded in Julia using the `RData` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb7bef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using RData, CSV, DataFrames, StatsBase, Gadfly, StatsModels, GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1707eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata = RData.load(\"../data/pension.RData\", convert = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9803434",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(rdata[\"pension\"]);\n",
    "categorical!(data, [:e401, :p401]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad21801",
   "metadata": {},
   "source": [
    "The data consist of 9,915 observations at the household level drawn from the 1991 Survey of Income and Program Participation (SIPP).  All the variables are referred to 1990. We use net financial assets (*net\\_tfa*) as the outcome variable, $Y$,  in our analysis. The net financial assets are computed as the sum of IRA balances, 401(k) balances, checking accounts, saving bonds, other interest-earning accounts, other interest-earning assets, stocks, and mutual funds less non mortgage debts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4294a363",
   "metadata": {},
   "source": [
    "Among the $9915$ individuals, $3682$ are eligible to participate in the program. The variable *e401* indicates eligibility and *p401* indicates participation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ca9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[!, \"n\"] .= 1;\n",
    "e_count = combine(groupby(data, :e401), :n => sum => :Freq);\n",
    "plot(e_count, color=:e401, y=:Freq, Geom.bar(position=:dodge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47875ab",
   "metadata": {},
   "source": [
    "Eligibility is highly associated with financial wealth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c5f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot(data, x=:net_tfa, xgroup=:e401, color=:e401, Geom.subplot_grid(Geom.density()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3569ee93",
   "metadata": {},
   "source": [
    "The unconditional APE of e401 is about $19559$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fea8fd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19559.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1 = data[data.e401 .== 0, :];\n",
    "e2 = data[data.e401 .== 1, :];\n",
    "round(mean(e2.net_tfa) - mean(e1.net_tfa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7edae1",
   "metadata": {},
   "source": [
    "Among the $3682$ individuals that are eligible, $2594$ decided to participate in the program. The unconditional APE of p401 is about $27372$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b90dd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27372.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = data[data.p401 .== 0, :];\n",
    "p2 = data[data.p401 .== 1, :];\n",
    "round(mean(p2.net_tfa) - mean(p1.net_tfa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a371c0",
   "metadata": {},
   "source": [
    "As discussed, these estimates are biadsed since they don't account for saver heterogeneity and endogeneity of participation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6aa6f7",
   "metadata": {},
   "source": [
    "## Estimating the ATE of 401(k) Eligibility on Financial Assets\n",
    "\n",
    "We first look at the treatment effect of e401 on net total financial assets. We give estimates of the ATE and ATT that corresponds to the linear model\n",
    "\n",
    "$$\n",
    "Y = D\\alpha + f(X)'\\beta + \\epsilon,\n",
    "$$\n",
    "\n",
    "where  $f(X)$  includes indicators of marital status, two-earner status, defined benefit pension status, IRA participation status, and home ownership status, and orthogonal polynomials of degrees 2, 4, 6 and 8 in family size, education, age and income, respectively. The dimensions of  $f(X)$  is 25.\n",
    "\n",
    "In the first step, we report estimates of the average treatment effect (ATE) of 401(k) eligibility on net financial assets both in the partially linear regression (PLR) model and in the interactive regression model (IRM) allowing for heterogeneous treatment effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e31db026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the `poly` function as provided by the documentation of the `StatsModels` package:\n",
    "\n",
    "# syntax: best practice to define a _new_ function\n",
    "poly(x, n) = x^n\n",
    "\n",
    "# type of model where syntax applies: here this applies to any model type\n",
    "const POLY_CONTEXT = Any\n",
    "\n",
    "# struct for behavior\n",
    "struct PolyTerm{T,D} <: AbstractTerm\n",
    "    term::T\n",
    "    deg::D\n",
    "end\n",
    "\n",
    "Base.show(io::IO, p::PolyTerm) = print(io, \"poly($(p.term), $(p.deg))\")\n",
    "\n",
    "# for `poly` use at run-time (outside @formula), return a schema-less PolyTerm\n",
    "poly(t::Symbol, d::Int) = PolyTerm(term(t), term(d))\n",
    "\n",
    "# for `poly` use inside @formula: create a schemaless PolyTerm and apply_schema\n",
    "function StatsModels.apply_schema(t::FunctionTerm{typeof(poly)},\n",
    "                                  sch::StatsModels.Schema,\n",
    "                                  Mod::Type{<:POLY_CONTEXT})\n",
    "    apply_schema(PolyTerm(t.args_parsed...), sch, Mod)\n",
    "end\n",
    "\n",
    "# apply_schema to internal Terms and check for proper types\n",
    "function StatsModels.apply_schema(t::PolyTerm,\n",
    "                                  sch::StatsModels.Schema,\n",
    "                                  Mod::Type{<:POLY_CONTEXT})\n",
    "    term = apply_schema(t.term, sch, Mod)\n",
    "    isa(term, ContinuousTerm) ||\n",
    "        throw(ArgumentError(\"PolyTerm only works with continuous terms (got $term)\"))\n",
    "    isa(t.deg, ConstantTerm) ||\n",
    "        throw(ArgumentError(\"PolyTerm degree must be a number (got $t.deg)\"))\n",
    "    PolyTerm(term, t.deg.n)\n",
    "end\n",
    "\n",
    "function StatsModels.modelcols(p::PolyTerm, d::NamedTuple)\n",
    "    col = modelcols(p.term, d)\n",
    "    reduce(hcat, [col.^n for n in 1:p.deg])\n",
    "end\n",
    "\n",
    "# the basic terms contained within a PolyTerm (for schema extraction)\n",
    "StatsModels.terms(p::PolyTerm) = terms(p.term)\n",
    "# names variables from the data that a PolyTerm relies on\n",
    "StatsModels.termvars(p::PolyTerm) = StatsModels.termvars(p.term)\n",
    "# number of columns in the matrix this term produces\n",
    "StatsModels.width(p::PolyTerm) = p.deg\n",
    "\n",
    "StatsBase.coefnames(p::PolyTerm) = coefnames(p.term) .* \"^\" .* string.(1:p.deg)\n",
    "\n",
    "# output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a4f8aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Constructing the Data\n",
    "\n",
    "formula_flex = @formula(net_tfa ~ e401 + poly(age, 6) + poly(inc, 8) + poly(educ, 4) + poly(fsize, 2) + marr + twoearn + db + pira + hown);\n",
    "formula_flex = apply_schema(formula_flex, schema(data));\n",
    "y, x = modelcols(formula_flex, data);\n",
    "y = Float64.(y)\n",
    "d = x[:, 1];\n",
    "x = x[:, Not(1)];\n",
    "size(x, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a5c51",
   "metadata": {},
   "source": [
    "## Partially Linear Regression Models (PLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db7d5c",
   "metadata": {},
   "source": [
    "We start using lasso to estimate the function $g_0$ and $m_0$ in the following PLR model:\n",
    "\n",
    "$$\n",
    "Y = D\\theta_0 + g_0(X) + \\zeta, E[\\zeta | D, X] = 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "D = m_0(X) + V, E[V| X] = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b6eb041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DML2_for_PLM (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLDataUtils, MLBase, Random, FixedEffectModels, GLMNet\n",
    "\n",
    "function DML2_for_PLM(x , d , y, dreg , yreg , nfold)\n",
    "    \n",
    "    # Num ob observations\n",
    "    nobser = size(x,1)\n",
    "    \n",
    "    # Define folds indices \n",
    "    foldid = collect(Kfold(size(x)[1], nfold))\n",
    "    \n",
    "    # Create array to save errors \n",
    "    ytil = ones(nobser)\n",
    "    dtil = ones(nobser)\n",
    "    \n",
    "    dl = convert(Matrix{Float64}, [(d .< 0.5) (d .>= 0.5)])\n",
    "    \n",
    "    # loop to save results\n",
    "    for i in 1:nfold\n",
    "        \n",
    "        # Lasso regression, excluding folds selected \n",
    "        dfit = dreg(x[foldid[i],:], dl[foldid[i], :])\n",
    "        yfit = yreg(x[foldid[i],:], y[foldid[i]])\n",
    "        \n",
    "        # Predict estimates using the \n",
    "        dhat = GLMNet.predict(dfit, x[Not(foldid[i]),:], outtype = :prob)\n",
    "        yhat = GLMNet.predict(yfit, x[Not(foldid[i]),:])\n",
    "        \n",
    "        # Save errors \n",
    "        dtil[Not(foldid[i])] = (d[Not(foldid[i])] - dhat)\n",
    "        ytil[Not(foldid[i])] = (y[Not(foldid[i])] - yhat)\n",
    "    end\n",
    "    \n",
    "    # Create dataframe \n",
    "    data = DataFrame(ytil = ytil, dtil = dtil)\n",
    "    \n",
    "    # OLS clustering at the County level\n",
    "    rfit = fit(LinearModel, reshape(dtil, nobser, 1), ytil)\n",
    "    # coef_est = coef(rfit)[2]\n",
    "    # se = FixedEffectModels.coeftable(rfit).cols[2][2]\n",
    "\n",
    "    # println(\" coef (se) = \", coef_est ,\"(\",se,\")\")\n",
    "    \n",
    "    return rfit, data;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8eaf8c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "─────────────────────────────────────────────────────────────\n",
       "      Coef.  Std. Error     t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "─────────────────────────────────────────────────────────────\n",
       "x1  6536.48     1250.23  5.23    <1e-06    4085.77    8987.19\n",
       "─────────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimating the PLR\n",
    "\n",
    "Random.seed!(123)\n",
    "dreg(x,d) = glmnetcv(x, d, nfolds = 5, Binomial())\n",
    "yreg(x,y) = glmnetcv(x, y, nfolds = 5)\n",
    "lasso_fit, lasso_data = DML2_for_PLM(x, d, y, dreg, yreg, 3);\n",
    "lasso_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3154c68",
   "metadata": {},
   "source": [
    "Let us check the predictive performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12af5812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55738.19944273792"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross-fitted RMSE: outcome\n",
    "lasso_y_rmse = sqrt(mean((lasso_data[!, 1] .- StatsBase.coef(lasso_fit)[1] * lasso_data[!, 2]) .^ 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9690ceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44775159135451154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31860816944024206"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross-fitted RMSE: treatment\n",
    "\n",
    "lasso_d_rmse = sqrt(mean(lasso_data[!, 2] .^ 2));\n",
    "\n",
    "println(lasso_d_rmse)\n",
    "\n",
    "# cross-fitted ce: treatment\n",
    "\n",
    "mean(ifelse.(d .- lasso_data[!, 2] .> 0.5, 1, 0) .!= d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d9f12",
   "metadata": {},
   "source": [
    "Then we repeat this proceedure for various machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e4dd3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DML2_RF (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forrest\n",
    "\n",
    "using DecisionTree\n",
    "\n",
    "function DML2_RF(z , d , y, dreg , yreg , nfold)\n",
    "    \n",
    "    # Num ob observations\n",
    "    nobser = size(z,1)\n",
    "    \n",
    "    # Define folds indices\n",
    "    foldid = collect(Kfold(size(z)[1], nfold))\n",
    "    \n",
    "    # Create array to save errors \n",
    "    ytil = ones(nobser)\n",
    "    dtil = ones(nobser)\n",
    "    \n",
    "    # loop to save results\n",
    "    for i in 1:nfold\n",
    "        dfit = dreg(z[foldid[i],:], d[foldid[i]])\n",
    "        yfit = yreg(z[foldid[i],:], y[foldid[i]])\n",
    "        dhat = apply_forest(dfit,z[Not(foldid[i]),:])\n",
    "        yhat = apply_forest(yfit,z[Not(foldid[i]),:])\n",
    "        dtil[Not(foldid[i])]   = (d[Not(foldid[i])] - dhat)\n",
    "        ytil[Not(foldid[i])]   = (y[Not(foldid[i])] - yhat)\n",
    "    end\n",
    "    \n",
    "    # Create dataframe \n",
    "    data = DataFrame(ytil = ytil, dtil = dtil)\n",
    "    \n",
    "    # OLS clustering at the County level\n",
    "    # rfit = reg(data, @formula(ytil ~ dtil))\n",
    "    rfit = fit(LinearModel, reshape(dtil, nobser, 1), ytil)\n",
    "    # coef_est = coef(rfit)[1]\n",
    "    # se = FixedEffectModels.coeftable(rfit).cols[2]\n",
    "\n",
    "    # println(\" coef (se) = \", coef_est ,\"(\",se,\")\")\n",
    "    \n",
    "    return rfit, data;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08779f60-5bb9-4294-ae30-253315d67fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Any}:\n",
       " [2, 4, 6, 8, 9, 11, 12, 13, 14, 16  …  9901, 9903, 9904, 9905, 9908, 9909, 9910, 9911, 9914, 9915]\n",
       " [1, 3, 5, 7, 8, 9, 10, 12, 13, 15  …  9900, 9902, 9903, 9905, 9906, 9907, 9910, 9912, 9913, 9915]\n",
       " [1, 2, 3, 4, 5, 6, 7, 10, 11, 14  …  9902, 9904, 9906, 9907, 9908, 9909, 9911, 9912, 9913, 9914]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldid = collect(Kfold(size(x)[1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d44ae5c2-3e29-43ff-b0d8-9635babffe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3305"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(foldid[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e0b4c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "─────────────────────────────────────────────────────────────\n",
       "      Coef.  Std. Error     t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "─────────────────────────────────────────────────────────────\n",
       "x1  8753.48      1214.5  7.21    <1e-12    6372.81    11134.2\n",
       "─────────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "\n",
    "dreg(x, d) = build_forest(d, x)\n",
    "yreg(x, y) = build_forest(y, x)\n",
    "\n",
    "rf_fit, rf_data = DML2_RF(x, d, y, dreg, yreg, 3);\n",
    "rf_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41017d38",
   "metadata": {},
   "source": [
    "We can compare the accuracy of this model to the model that was estimated with lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a9fc5dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56226.390040126214\n",
      "0.46496145990094206"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3447302067574382"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rf_y_rmse = sqrt(mean((rf_data[!, 1] .- StatsBase.coef(rf_fit)[1] * rf_data[!, 2]) .^ 2))\n",
    "rf_d_rmse = sqrt(mean(rf_data[!, 2] .^ 2))\n",
    "\n",
    "println(rf_y_rmse)\n",
    "\n",
    "println(rf_d_rmse)\n",
    "\n",
    "mean(ifelse.(d .- rf_data[!, 2] .> 0.5, 1, 0) .!= d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c50c4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DML2_Tree (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trees\n",
    "\n",
    "function DML2_Tree(z , d , y, dreg , yreg , nfold)\n",
    "    \n",
    "    # Num ob observations\n",
    "    nobser = size(z,1)\n",
    "    \n",
    "    # Define folds indices\n",
    "    foldid = collect(Kfold(size(z)[1], nfold))\n",
    "    \n",
    "    # Create array to save errors \n",
    "    ytil = ones(nobser)\n",
    "    dtil = ones(nobser)\n",
    "    \n",
    "    # loop to save results\n",
    "    for i in 1:nfold\n",
    "        dfit = dreg(z[foldid[i],:], d[foldid[i]])\n",
    "        yfit = yreg(z[foldid[i],:], y[foldid[i]])\n",
    "        dhat = apply_tree(dfit,z[Not(foldid[i]),:])\n",
    "        yhat = apply_tree(yfit,z[Not(foldid[i]),:])\n",
    "        dtil[Not(foldid[i])]   = (d[Not(foldid[i])] - dhat)\n",
    "        ytil[Not(foldid[i])]   = (y[Not(foldid[i])] - yhat)\n",
    "    end\n",
    "    \n",
    "    # Create dataframe \n",
    "    data = DataFrame(ytil = ytil, dtil = dtil)\n",
    "    \n",
    "    # OLS clustering at the County level\n",
    "    rfit = fit(LinearModel, reshape(dtil, nobser, 1), ytil)\n",
    "    # coef_est = coef(rfit)[1]\n",
    "    # se = FixedEffectModels.coeftable(rfit).cols[2]\n",
    "\n",
    "    # println(\" coef (se) = \", coef_est ,\"(\",se,\")\")\n",
    "    \n",
    "    return rfit, data;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35e9355a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "─────────────────────────────────────────────────────────────\n",
       "      Coef.  Std. Error     t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "─────────────────────────────────────────────────────────────\n",
       "x1  8205.33     1320.35  6.21    <1e-09    5617.18    10793.5\n",
       "─────────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "\n",
    "dreg(x, d) = build_tree(d, x, 0, 30, 7, 20, 0.01)\n",
    "yreg(x, y) = build_tree(y, x, 0, 30, 7, 20, 0.01)\n",
    "\n",
    "tree_fit, tree_data = DML2_Tree(x, d, y, dreg, yreg, 3);\n",
    "tree_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e7f4c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59676.120532917645\n",
      "0.45392774217158094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34664649520927887"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_y_rmse = sqrt(mean((tree_data[!, 1] .- StatsBase.coef(tree_fit)[1] * tree_data[!, 2]) .^ 2))\n",
    "tree_d_rmse = sqrt(mean(tree_data[!, 2] .^ 2))\n",
    "\n",
    "println(tree_y_rmse)\n",
    "\n",
    "println(tree_d_rmse)\n",
    "\n",
    "mean(ifelse.(d .- tree_data[!, 2] .> 0.5, 1, 0) .!= d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5be2f10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DML2_Boost (generic function with 1 method)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boosting\n",
    "\n",
    "using XGBoost\n",
    "\n",
    "function DML2_Boost(z , d , y, dreg , yreg , nfold)\n",
    "    \n",
    "    # Num ob observations\n",
    "    nobser = size(z,1)\n",
    "    \n",
    "    # Define folds indices\n",
    "    foldid = collect(Kfold(size(z)[1], nfold))\n",
    "    \n",
    "    # Create array to save errors \n",
    "    ytil = ones(nobser)\n",
    "    dtil = ones(nobser)\n",
    "    \n",
    "    # loop to save results\n",
    "    for i in 1:nfold\n",
    "        dfit = dreg(z[foldid[i], :], d[foldid[i]])\n",
    "        yfit = yreg(z[foldid[i], :], y[foldid[i]])\n",
    "        dhat = XGBoost.predict(dfit, z[Not(foldid[i]), :])\n",
    "        yhat = XGBoost.predict(yfit, z[Not(foldid[i]), :])\n",
    "        dtil[Not(foldid[i])]   = (d[Not(foldid[i])] - dhat)\n",
    "        ytil[Not(foldid[i])]   = (y[Not(foldid[i])] - yhat)\n",
    "    end\n",
    "    \n",
    "    # Create dataframe \n",
    "    data = DataFrame(ytil = ytil, dtil = dtil)\n",
    "    \n",
    "    # OLS clustering at the County level\n",
    "    rfit = fit(LinearModel, reshape(dtil, nobser, 1), ytil)\n",
    "    # coef_est = coef(rfit)[1]\n",
    "    # se = FixedEffectModels.coeftable(rfit).cols[2]\n",
    "\n",
    "    # println(\" coef (se) = \", coef_est ,\"(\",se,\")\")\n",
    "    \n",
    "    return rfit, data;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8158da73-e767-4a67-ab05-dbd30de1236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-logloss:0.62814512403397260\n",
      "[2]\ttrain-logloss:0.59143994323424598\n",
      "[3]\ttrain-logloss:0.56518430078399340\n",
      "[4]\ttrain-logloss:0.54780421759510545\n",
      "[5]\ttrain-logloss:0.53631460053477098\n",
      "[1]\ttrain-rmse:50805.90975126884586643\n",
      "[2]\ttrain-rmse:45118.49221384151314851\n",
      "[3]\ttrain-rmse:41081.95237535334308632\n",
      "[4]\ttrain-rmse:38870.17674265824462054\n",
      "[5]\ttrain-rmse:36808.78356694047397468\n",
      "[1]\ttrain-logloss:0.62360870221829812\n",
      "[2]\ttrain-logloss:0.58373274581785828\n",
      "[3]\ttrain-logloss:0.55822905044151683\n",
      "[4]\ttrain-logloss:0.54074468446977197\n",
      "[5]\ttrain-logloss:0.52591642424712204\n",
      "[1]\ttrain-rmse:60903.80710911499772919\n",
      "[2]\ttrain-rmse:54099.31892967657768168\n",
      "[3]\ttrain-rmse:49576.11850658095499966\n",
      "[4]\ttrain-rmse:46556.65778888474596897\n",
      "[5]\ttrain-rmse:43876.91871598918805830\n",
      "[1]\ttrain-logloss:0.62742250420592738\n",
      "[2]\ttrain-logloss:0.58906182237814853\n",
      "[3]\ttrain-logloss:0.56361801188760374\n",
      "[4]\ttrain-logloss:0.54553039387071367\n",
      "[5]\ttrain-logloss:0.53199355807634419\n",
      "[1]\ttrain-rmse:58283.80948005586833460\n",
      "[2]\ttrain-rmse:52014.92906220545410179\n",
      "[3]\ttrain-rmse:47384.55433909337443765\n",
      "[4]\ttrain-rmse:44222.83196772700466681\n",
      "[5]\ttrain-rmse:42066.38531371586577734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "─────────────────────────────────────────────────────────────\n",
       "      Coef.  Std. Error     t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "─────────────────────────────────────────────────────────────\n",
       "x1  8469.37     1274.65  6.64    <1e-10    5970.79    10967.9\n",
       "─────────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "\n",
    "dreg(x, d) = xgboost(x, 5, label = d, objective = \"binary:logistic\", eval_metric = \"logloss\");\n",
    "yreg(x, y) = xgboost(x, 5, label = y);\n",
    "\n",
    "boost_fit, boost_data = DML2_Boost(x, d, y, dreg, yreg, 3);\n",
    "boost_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2c85290-29bc-484f-a642-62b115400c03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56917.35363267462\n",
      "0.44846539610140396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3179021684316692"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_y_rmse = sqrt(mean((boost_data[!, 1] .- StatsBase.coef(boost_fit)[1] * boost_data[!, 2]) .^ 2))\n",
    "boost_d_rmse = sqrt(mean(boost_data[!, 2] .^ 2))\n",
    "\n",
    "println(boost_y_rmse)\n",
    "\n",
    "println(boost_d_rmse)\n",
    "\n",
    "mean(ifelse.(d .- boost_data[!, 2] .> 0.5, 1, 0) .!= d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb1708-db3b-485b-9e1f-60a23536da22",
   "metadata": {},
   "source": [
    "Let's sum up the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879743a-7651-4c27-a057-5028abfe25f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Statistic</th><th>Lasso</th><th>RF</th><th>Trees</th><th>Boosting</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>4 rows × 5 columns</p><tr><th>1</th><td>Estimate</td><td>6536.48</td><td>8753.48</td><td>8205.33</td><td>8469.37</td></tr><tr><th>2</th><td>Std.Error</td><td>1250.23</td><td>1214.5</td><td>1320.35</td><td>1274.65</td></tr><tr><th>3</th><td>RMSE Y</td><td>55738.2</td><td>56226.4</td><td>59676.1</td><td>56917.4</td></tr><tr><th>4</th><td>RMSE D</td><td>0.447752</td><td>0.464961</td><td>0.453928</td><td>0.448465</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& Statistic & Lasso & RF & Trees & Boosting\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & Estimate & 6536.48 & 8753.48 & 8205.33 & 8469.37 \\\\\n",
       "\t2 & Std.Error & 1250.23 & 1214.5 & 1320.35 & 1274.65 \\\\\n",
       "\t3 & RMSE Y & 55738.2 & 56226.4 & 59676.1 & 56917.4 \\\\\n",
       "\t4 & RMSE D & 0.447752 & 0.464961 & 0.453928 & 0.448465 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m4×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Statistic \u001b[0m\u001b[1m Lasso        \u001b[0m\u001b[1m RF           \u001b[0m\u001b[1m Trees        \u001b[0m\u001b[1m Boosting     \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m String    \u001b[0m\u001b[90m Float64      \u001b[0m\u001b[90m Float64      \u001b[0m\u001b[90m Float64      \u001b[0m\u001b[90m Float64      \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────────\n",
       "   1 │ Estimate    6536.48       8753.48       8205.33       8469.37\n",
       "   2 │ Std.Error   1250.23       1214.5        1320.35       1274.65\n",
       "   3 │ RMSE Y     55738.2       56226.4       59676.1       56917.4\n",
       "   4 │ RMSE D         0.447752      0.464961      0.453928      0.448465"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(Statistic = [\"Estimate\", \"Std.Error\", \"RMSE Y\", \"RMSE D\"], \n",
    "    Lasso = [StatsBase.coef(lasso_fit)[1], sqrt(vcov(lasso_fit)[1]), lasso_y_rmse, lasso_d_rmse], \n",
    "    RF = [StatsBase.coef(rf_fit)[1], sqrt(vcov(rf_fit)[1]), rf_y_rmse, rf_d_rmse], \n",
    "    Trees = [StatsBase.coef(tree_fit)[1], sqrt(vcov(tree_fit)[1]), tree_y_rmse, tree_d_rmse], \n",
    "    Boosting = [StatsBase.coef(boost_fit)[1], sqrt(vcov(boost_fit)[1]), boost_y_rmse, boost_d_rmse])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74767cf7-0955-4ecd-85b2-fbe383f1ec1b",
   "metadata": {},
   "source": [
    "## Interactive Regression Model (IRM)\n",
    "\n",
    "Next, we consider estimation of average treatment effects when treatment effects are fully heterogeneous:\n",
    "\n",
    "$$\n",
    "Y = g_0(D, X) + U, E[U | X, D] = 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "D = m_0(X) + V, E[V | X] = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d91d50c7-a069-4f4e-abd7-10212d3e8d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IRM_Lasso (generic function with 2 methods)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function based off of: https://github.com/DoubleML/doubleml-for-r/blob/f00d62c722a2b1e37c01b7f7f772e9d07f452a98/R/double_ml_irm.R\n",
    "#                        https://github.com/DoubleML/doubleml-for-r/blob/f00d62c722a2b1e37c01b7f7f772e9d07f452a98/R/double_ml_plr.R\n",
    "\n",
    "# Function takes x, y, d, learners, and nfolds\n",
    "\n",
    "function IRM_Lasso(x, y, d, ml_g, ml_m, nfold, trimming_threshold = 1e-12)\n",
    "    \n",
    "    # Sample size\n",
    "    nobser = size(x, 1)\n",
    "    \n",
    "    # Fold indexes\n",
    "    foldid = collect(Kfold(size(x)[1], nfold))\n",
    "    \n",
    "    # Initialize vectors for predictions\n",
    "    y1_hat = ones(nobser)\n",
    "    y0_hat = ones(nobser)\n",
    "    d_hat = ones(nobser)\n",
    "    dl = convert(Matrix{Float64}, [(d .< 0.5) (d .>= 0.5)])\n",
    "    \n",
    "    # Apply learners to y_0, y_1 and d separately\n",
    "    for i in 1:nfold\n",
    "        # Create y_0 and y_1 for this fold\n",
    "        mask = findall(==(1), d[foldid[i]])\n",
    "        smp_1 = foldid[i][mask]\n",
    "        smp_0 = foldid[i][Not(mask)]\n",
    "        \n",
    "        # Model Learning\n",
    "        g0_hat = ml_g(x[smp_0, :], y[smp_0])\n",
    "        g1_hat = ml_g(x[smp_1, :], y[smp_1])\n",
    "        m_hat = ml_m(x[foldid[i], :], dl[foldid[i], :])\n",
    "        \n",
    "        # Predict: g0_hat, g1_hat, m_hat\n",
    "        d_hat[Not(foldid[i])] = GLMNet.predict(m_hat, x[Not(foldid[i]), :], outtype = :prob)\n",
    "        y0_hat[Not(foldid[i])] = GLMNet.predict(g0_hat, x[Not(foldid[i]), :])\n",
    "        y1_hat[Not(foldid[i])] = GLMNet.predict(g1_hat, x[Not(foldid[i]), :])\n",
    "    \n",
    "    end\n",
    "    \n",
    "    # Residuals: u0_hat, u1_hat, no need for residual in d\n",
    "    u0_hat = y .- y0_hat\n",
    "    u1_hat = y .- y1_hat\n",
    "    \n",
    "    # Trimming\n",
    "    d_hat[d_hat .< trimming_threshold] .= trimming_threshold\n",
    "    d_hat[d_hat .> (1 - trimming_threshold)] .= 1 - trimming_threshold\n",
    "\n",
    "    # Compute regression terms:\n",
    "    # Left side: y1_hat - y0_hat + d * u1_hat / m_hat - (1 - d) * u0_hat / (1 - m_hat)\n",
    "    psi_b = y1_hat .- y0_hat .+ d .* u1_hat ./ d_hat - (1 .- d) .* u0_hat ./ (1 .- d_hat)\n",
    "    \n",
    "    # Right side: All ones\n",
    "    psi_a = reshape(ones(nobser), nobser, 1)\n",
    "    \n",
    "    # Regression with fit(LinearModel, ...)\n",
    "    rfit = fit(LinearModel, psi_a, psi_b)\n",
    "    \n",
    "    # Generate data matrix for output\n",
    "    u_hat = d .* u1_hat + (1 .- d) .* u0_hat\n",
    "    d_til = d .- d_hat\n",
    "    data = DataFrame(u_hat = u_hat, d_til = d_til)\n",
    "    \n",
    "    # Function outputs residual data and ATE\n",
    "    return rfit, data;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e372cf6a-4c3f-4a07-90cd-d9343510f72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "────────────────────────────────────────────────────────────\n",
       "     Coef.  Std. Error     t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "────────────────────────────────────────────────────────────\n",
       "x1  1527.2     4241.99  0.36    0.7188   -6787.97    9842.37\n",
       "────────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "ml_m(x, d) = glmnetcv(x, d, nfolds = 5, Binomial())\n",
    "ml_g(x, y) = glmnetcv(x, y, nfolds = 5)\n",
    "lasso_fit, lasso_data = IRM_Lasso(x, y, d, ml_g, ml_m, 3, 0.01);\n",
    "lasso_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2503cb2d-c2db-4887-b51f-3745c7e642af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IRM_Forest (generic function with 1 method)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function IRM_Forest(x, y, d, ml_g, ml_m, nfold)\n",
    "    \n",
    "    # Sample size\n",
    "    nobser = size(x, 1)\n",
    "    \n",
    "    # Fold indexes\n",
    "    foldid = collect(Kfold(size(x)[1], nfold))\n",
    "    \n",
    "    # Initialize vectors for predictions\n",
    "    y1_hat = ones(nobser)\n",
    "    y0_hat = ones(nobser)\n",
    "    d_hat = ones(nobser)\n",
    "    \n",
    "    # Apply learners to y_0, y_1 and d separately\n",
    "    for i in 1:nfold\n",
    "        # Create y_0 and y_1 for this fold\n",
    "        mask = findall(==(1), d[foldid[i]])\n",
    "        smp_1 = foldid[i][mask]\n",
    "        smp_0 = foldid[i][Not(mask)]\n",
    "        \n",
    "        # Model Learning\n",
    "        g0_hat = ml_g(x[smp_0, :], y[smp_0])\n",
    "        g1_hat = ml_g(x[smp_1, :], y[smp_1])\n",
    "        m_hat = ml_m(x[foldid[i], :], d[foldid[i]])\n",
    "        \n",
    "        # Predict: g0_hat, g1_hat, m_hat\n",
    "        d_hat[Not(foldid[i])] = apply_forest(m_hat, x[Not(foldid[i]), :])\n",
    "        y0_hat[Not(foldid[i])] = apply_forest(g0_hat, x[Not(foldid[i]), :])\n",
    "        y1_hat[Not(foldid[i])] = apply_forest(g1_hat, x[Not(foldid[i]), :])\n",
    "    \n",
    "    end\n",
    "    \n",
    "    # Residuals: u0_hat, u1_hat, no need for residual in d\n",
    "    u0_hat = y .- y0_hat\n",
    "    u1_hat = y .- y1_hat\n",
    "    \n",
    "    # Compute regression terms:\n",
    "    # Left side: y1_hat - y0_hat + d * u1_hat / m_hat \n",
    "    #            - (1 - d) * u0_hat / (1 - m_hat)\n",
    "    psi_b = y1_hat .- y0_hat .+ d .* u1_hat ./ d_hat - (1 .- d) .* u0_hat ./ (1 .- d_hat)\n",
    "    \n",
    "    # Right side: All ones\n",
    "    psi_a = reshape(ones(nobser), nobser, 1)\n",
    "    \n",
    "    # Regression with fit(LinearModel, ...)\n",
    "    rfit = fit(LinearModel, psi_a, psi_b)\n",
    "    \n",
    "    # Generate data matrix for output\n",
    "    u_hat = d .* u1_hat + (1 .- d) .* u0_hat\n",
    "    d_til = d .- d_hat\n",
    "    data = DataFrame(u_hat = u_hat, d_til = d_til)\n",
    "    \n",
    "    # Function outputs residual data and ATE\n",
    "    return rfit, data, psi_a, psi_b;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6991237d-5c2e-4a80-bdd5-0a493b448b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "──────────────────────────────────────────────────────────\n",
       "    Coef.  Std. Error    t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "──────────────────────────────────────────────────────────\n",
       "x1    NaN         NaN  NaN       NaN        NaN        NaN\n",
       "──────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "\n",
    "ml_m(x, d) = build_forest(d, x)\n",
    "ml_g(x, y) = build_forest(y, x)\n",
    "\n",
    "rf_fit, rf_data, psi_a, psi_b = IRM_Forest(x, y, d, ml_g, ml_m, 3);\n",
    "rf_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4984da67-3962-46ca-9424-249b1b059ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915-element Vector{Float64}:\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       "  Inf\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       "   ⋮\n",
       "  Inf\n",
       "  Inf\n",
       " -Inf\n",
       "  Inf\n",
       "  Inf\n",
       " -Inf\n",
       "  Inf\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " -Inf\n",
       " -Inf"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "690eb9ed-b4f3-4103-aafc-0f59dec78a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IRM_Tree (generic function with 2 methods)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function IRM_Tree(x, y, d, ml_g, ml_m, nfold)\n",
    "    \n",
    "    # Sample size\n",
    "    nobser = size(x, 1)\n",
    "    \n",
    "    # Fold indexes\n",
    "    foldid = collect(Kfold(size(x)[1], nfold))\n",
    "    \n",
    "    # Initialize vectors for predictions\n",
    "    y1_hat = ones(nobser)\n",
    "    y0_hat = ones(nobser)\n",
    "    d_hat = ones(nobser)\n",
    "    \n",
    "    # Apply learners to y_0, y_1 and d separately\n",
    "    for i in 1:nfold\n",
    "        # Create y_0 and y_1 for this fold\n",
    "        mask = findall(==(1), d[foldid[i]])\n",
    "        smp_1 = foldid[i][mask]\n",
    "        smp_0 = foldid[i][Not(mask)]\n",
    "        \n",
    "        # Model Learning\n",
    "        g0_hat = ml_g(x[smp_0, :], y[smp_0])\n",
    "        g1_hat = ml_g(x[smp_1, :], y[smp_1])\n",
    "        m_hat = ml_m(x[foldid[i], :], d[foldid[i]])\n",
    "        \n",
    "        # Predict: g0_hat, g1_hat, m_hat\n",
    "        d_hat[Not(foldid[i])] = apply_tree(m_hat, x[Not(foldid[i]), :])\n",
    "        y0_hat[Not(foldid[i])] = apply_tree(g0_hat, x[Not(foldid[i]), :])\n",
    "        y1_hat[Not(foldid[i])] = apply_tree(g1_hat, x[Not(foldid[i]), :])\n",
    "    \n",
    "    end\n",
    "    \n",
    "    # Residuals: u0_hat, u1_hat, no need for residual in d\n",
    "    u0_hat = y .- y0_hat\n",
    "    u1_hat = y .- y1_hat\n",
    "    \n",
    "    # Compute regression terms:\n",
    "    # Left side: y1_hat - y0_hat + d * u1_hat / m_hat \n",
    "    #            - (1 - d) * u0_hat / (1 - m_hat)\n",
    "    psi_b = y1_hat .- y0_hat .+ d .* u1_hat ./ d_hat - (1 .- d) .* u0_hat ./ (1 .- d_hat)\n",
    "    \n",
    "    # Right side: All ones\n",
    "    psi_a = reshape(ones(nobser), nobser, 1)\n",
    "    \n",
    "    # Regression with fit(LinearModel, ...)\n",
    "    rfit = fit(LinearModel, psi_a, psi_b)\n",
    "    \n",
    "    # Generate data matrix for output\n",
    "    u_hat = d .* u1_hat + (1 .- d) .* u0_hat\n",
    "    d_til = d .- d_hat\n",
    "    data = DataFrame(u_hat = u_hat, d_til = d_til)\n",
    "    \n",
    "    # Function outputs residual data and ATE\n",
    "    return rfit, data;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3b6d547d-489d-4e7f-bc5f-b4b07481cf48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "─────────────────────────────────────────────────────────────\n",
       "      Coef.  Std. Error     t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "─────────────────────────────────────────────────────────────\n",
       "x1  7137.53     1212.17  5.89    <1e-08    4761.43    9513.63\n",
       "─────────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "\n",
    "ml_m(x, d) = build_tree(d, x, 0, 30, 7, 20, 0.01)\n",
    "ml_g(x, y) = build_tree(y, x, 0, 30, 7, 20, 0.01)\n",
    "\n",
    "tree_fit, tree_data = IRM_Tree(x, y, d, ml_g, ml_m, 3);\n",
    "tree_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "469794ca-0d7f-4a13-9f50-650da25ebdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IRM_Boost (generic function with 2 methods)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function IRM_Boost(x, y, d, ml_g, ml_m, nfold, trimming_threshold = 1e-12)\n",
    "    \n",
    "    # Sample size\n",
    "    nobser = size(x, 1)\n",
    "    \n",
    "    # Fold indexes\n",
    "    foldid = collect(Kfold(size(x)[1], nfold))\n",
    "    \n",
    "    # Initialize vectors for predictions\n",
    "    y1_hat = ones(nobser)\n",
    "    y0_hat = ones(nobser)\n",
    "    d_hat = ones(nobser)\n",
    "    \n",
    "    # Apply learners to y_0, y_1 and d separately\n",
    "    for i in 1:nfold\n",
    "        # Create y_0 and y_1 for this fold\n",
    "        mask = findall(==(1), d[foldid[i]])\n",
    "        smp_1 = foldid[i][mask]\n",
    "        smp_0 = foldid[i][Not(mask)]\n",
    "        \n",
    "        # Model Learning\n",
    "        g0_hat = ml_g(x[smp_0, :], y[smp_0])\n",
    "        g1_hat = ml_g(x[smp_1, :], y[smp_1])\n",
    "        m_hat = ml_m(x[foldid[i], :], d[foldid[i]])\n",
    "        \n",
    "        # Predict: g0_hat, g1_hat, m_hat\n",
    "        d_hat[Not(foldid[i])] = XGBoost.predict(m_hat, x[Not(foldid[i]), :])\n",
    "        y0_hat[Not(foldid[i])] = XGBoost.predict(g0_hat, x[Not(foldid[i]), :])\n",
    "        y1_hat[Not(foldid[i])] = XGBoost.predict(g1_hat, x[Not(foldid[i]), :])\n",
    "    \n",
    "    end\n",
    "    \n",
    "    # Residuals: u0_hat, u1_hat, no need for residual in d\n",
    "    u0_hat = y .- y0_hat\n",
    "    u1_hat = y .- y1_hat\n",
    "    \n",
    "    # Trimming\n",
    "    d_hat[d_hat .< trimming_threshold] .= trimming_threshold\n",
    "    d_hat[d_hat .> (1 - trimming_threshold)] .= 1 - trimming_threshold\n",
    "\n",
    "    # Compute regression terms:\n",
    "    # Left side: y1_hat - y0_hat + d * u1_hat / m_hat \n",
    "    #            - (1 - d) * u0_hat / (1 - m_hat)\n",
    "    psi_b = y1_hat .- y0_hat .+ d .* u1_hat ./ d_hat - (1 .- d) .* u0_hat ./ (1 .- d_hat)\n",
    "    \n",
    "    # Right side: All ones\n",
    "    psi_a = reshape(ones(nobser), nobser, 1)\n",
    "    \n",
    "    # Regression with fit(LinearModel, ...)\n",
    "    rfit = fit(LinearModel, psi_a, psi_b)\n",
    "    \n",
    "    # Generate data matrix for output\n",
    "    u_hat = d .* u1_hat + (1 .- d) .* u0_hat\n",
    "    d_til = d .- d_hat\n",
    "    data = DataFrame(u_hat = u_hat, d_til = d_til)\n",
    "    \n",
    "    # Function outputs residual data and ATE\n",
    "    return rfit, data, psi_a, psi_b;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "54003465-bb1c-4ba0-9d95-7346bcac9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:39672.55862477455229964\n",
      "[2]\ttrain-rmse:33829.13558642176212743\n",
      "[3]\ttrain-rmse:30258.46958888950393884\n",
      "[4]\ttrain-rmse:27771.90457050152326701\n",
      "[5]\ttrain-rmse:25801.52380595109571004\n",
      "[1]\ttrain-rmse:63219.98147018595773261\n",
      "[2]\ttrain-rmse:54416.80836068426287966\n",
      "[3]\ttrain-rmse:47289.68965621231473051\n",
      "[4]\ttrain-rmse:42172.63499183114618063\n",
      "[5]\ttrain-rmse:37971.38256453725625761\n",
      "[1]\ttrain-logloss:0.62814512403397260\n",
      "[2]\ttrain-logloss:0.59143994323424598\n",
      "[3]\ttrain-logloss:0.56518430078399340\n",
      "[4]\ttrain-logloss:0.54780421759510545\n",
      "[5]\ttrain-logloss:0.53631460053477098\n",
      "[1]\ttrain-rmse:50636.30439475246384973\n",
      "[2]\ttrain-rmse:43745.13860136931907618\n",
      "[3]\ttrain-rmse:38834.36631127876171377\n",
      "[4]\ttrain-rmse:35144.75589004970242968\n",
      "[5]\ttrain-rmse:32663.59513701907417271\n",
      "[1]\ttrain-rmse:73605.79290901409694925\n",
      "[2]\ttrain-rmse:65207.46431023348122835\n",
      "[3]\ttrain-rmse:58405.75253156274266075\n",
      "[4]\ttrain-rmse:53038.06686622917186469\n",
      "[5]\ttrain-rmse:49754.44319479364639847\n",
      "[1]\ttrain-logloss:0.62360870221829812\n",
      "[2]\ttrain-logloss:0.58373274581785828\n",
      "[3]\ttrain-logloss:0.55822905044151683\n",
      "[4]\ttrain-logloss:0.54074468446977197\n",
      "[5]\ttrain-logloss:0.52591642424712204\n",
      "[1]\ttrain-rmse:50005.24054598450311460\n",
      "[2]\ttrain-rmse:43400.83870381317683496\n",
      "[3]\ttrain-rmse:38966.11923426424618810\n",
      "[4]\ttrain-rmse:35379.46674831327254651\n",
      "[5]\ttrain-rmse:32324.18250056869874243\n",
      "[1]\ttrain-rmse:69200.65689350117463619\n",
      "[2]\ttrain-rmse:60404.73848133030696772\n",
      "[3]\ttrain-rmse:54678.29552286212856416\n",
      "[4]\ttrain-rmse:50607.39462431120045949\n",
      "[5]\ttrain-rmse:47160.13022250941139646\n",
      "[1]\ttrain-logloss:0.62742250420592738\n",
      "[2]\ttrain-logloss:0.58906182237814853\n",
      "[3]\ttrain-logloss:0.56361801188760374\n",
      "[4]\ttrain-logloss:0.54553039387071367\n",
      "[5]\ttrain-logloss:0.53199355807634419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "─────────────────────────────────────────────────────────────\n",
       "      Coef.  Std. Error     t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "─────────────────────────────────────────────────────────────\n",
       "x1  7955.78      1175.3  6.77    <1e-10    5651.95    10259.6\n",
       "─────────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "\n",
    "ml_m(x, d) = xgboost(x, 5, label = d, objective = \"binary:logistic\", eval_metric = \"logloss\");\n",
    "ml_g(x, y) = xgboost(x, 5, label = y);\n",
    "\n",
    "boost_fit, boost_data, psi_a, psi_b = IRM_Boost(x, y, d, ml_g, ml_m, 3, 0.01);\n",
    "boost_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfae6d-3eab-4e3b-977c-dfdf64079131",
   "metadata": {},
   "source": [
    "## Local Average Treatment Effects of 401(k) Participation on Net Financial Assets\n",
    "\n",
    "## Interactive IV Model (IIVM)\n",
    "\n",
    "Now, we consider estimation of local average treatment effects (LATE) with the binary instrument 401. As before, $Y$ denotes the outcome `net_tfa`, and $X$ is the vector of covariates. Here, the structural equation model is:\n",
    "\n",
    "$$\n",
    "Y = g_0(Z, X) + U, E[U | Z, X] = 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "D = r_0(Z, X) + V, E[V | Z, X] = 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z = m_0(X) + \\zeta, E[\\zeta | X] = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "be67a712-26a4-443f-a0cd-6f71bdf19af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula_iivm = @formula(net_tfa ~ p401 + e401 + poly(age, 6) + poly(inc, 8) + poly(educ, 4) + poly(fsize, 2) + marr + twoearn + db + pira + hown);\n",
    "formula_iivm = apply_schema(formula_iivm, schema(data));\n",
    "y, x = modelcols(formula_iivm, data);\n",
    "d = Integer.(x[:, 2]);\n",
    "z = Integer.(x[:, 1]);\n",
    "x = x[:, Not([1, 2])];\n",
    "size(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "08a7ad23-b9bf-45ac-8567-90e004342511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IIVM_Lasso (generic function with 2 methods)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function IIVM_Lasso(x, y, d, z, ml_g, ml_r, ml_m, nfold, trimming_threshold = 1e-12)\n",
    "    \n",
    "    # Sample size\n",
    "    nobser = size(x, 1)\n",
    "    \n",
    "    # Fold indexes\n",
    "    foldid = collect(Kfold(size(x)[1], nfold))\n",
    "    \n",
    "    # Initialize vectors for predictions\n",
    "    y0_hat = ones(nobser)\n",
    "    y1_hat = ones(nobser)\n",
    "    d0_hat = zeros(nobser)\n",
    "    d1_hat = ones(nobser)\n",
    "    z_hat = ones(nobser)\n",
    "    \n",
    "    dl = convert(Matrix{Float64}, [(d .< 0.5) (d .>= 0.5)])\n",
    "    zl = convert(Matrix{Float64}, [(z .< 0.5) (z .>= 0.5)])\n",
    "    \n",
    "    # Apply learners to y_0, y_1, d_1, d_2 and d separately\n",
    "    for i in 1:nfold\n",
    "        # Create y_0 and y_1 for this fold\n",
    "        mask = findall(==(1), z[foldid[i]])\n",
    "        smp_1 = foldid[i][mask]\n",
    "        smp_0 = foldid[i][Not(mask)]\n",
    "        \n",
    "        # Model Learning\n",
    "        g0_hat = ml_g(x[smp_0, :], y[smp_0])\n",
    "        g1_hat = ml_g(x[smp_1, :], y[smp_1])\n",
    "        r1_hat = ml_r(x[smp_1, :], dl[smp_1, :])\n",
    "        m_hat = ml_m(x[foldid[i], :], zl[foldid[i], :])\n",
    "        \n",
    "        # Predict: g0_hat, g1_hat, m_hat\n",
    "        d1_hat[Not(foldid[i])] = GLMNet.predict(r1_hat, x[Not(foldid[i]), :], outtype = :prob)\n",
    "        y0_hat[Not(foldid[i])] = GLMNet.predict(g0_hat, x[Not(foldid[i]), :])\n",
    "        y1_hat[Not(foldid[i])] = GLMNet.predict(g1_hat, x[Not(foldid[i]), :])\n",
    "        z_hat[Not(foldid[i])] = GLMNet.predict(m_hat, x[Not(foldid[i]), :], outtype = :prob)\n",
    "    \n",
    "    end\n",
    "    \n",
    "    \n",
    "    # Residuals: u0_hat, u1_hat, w0_hat, and w1_hat; no need for residual in z\n",
    "    u0_hat = y .- y0_hat\n",
    "    u1_hat = y .- y1_hat\n",
    "    w0_hat = d .- d0_hat\n",
    "    w1_hat = d .- d1_hat\n",
    "    \n",
    "    # Compute regression terms:\n",
    "    # Left side: y1_hat - y0_hat + z * (u1_hat) / z_hat - (1 - z) * u0_hat / (1 - z_hat)\n",
    "    psi_b = y1_hat .- y0_hat .+ z .* u1_hat ./ z_hat .- (1 .- z) .* u0_hat ./ (1 .- z_hat)\n",
    "    \n",
    "    # Right side: d1_hat - d0_hat + z * (w1_hat) / z_hat - (1 - z) * w0_hat / (1 - z_hat)\n",
    "    psi_a = d1_hat .- d0_hat .+ z .* w1_hat ./ z_hat .- (1 .- z) .* w0_hat ./ (1 .- z_hat)\n",
    "    \n",
    "    # Regression with fit(LinearModel, ...)\n",
    "    rfit = fit(LinearModel, reshape(psi_a, nobser, 1), psi_b)\n",
    "    \n",
    "    # Generate data matrix for output\n",
    "    u_hat = z .* u1_hat + (1 .- z) .* u0_hat\n",
    "    d_til = z .* w1_hat + (1 .- z) .* w0_hat\n",
    "    data = DataFrame(u_hat = u_hat, d_til = d_til)\n",
    "    \n",
    "    # Function outputs residual data and ATE\n",
    "    return rfit, data, psi_a, psi_b;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f6dee7ce-6a90-409f-9401-2f1a4592df9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching glmnet!(::Matrix{Float64}, ::Vector{Float64}, ::Binomial{Float64}; weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], offsets=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\u001b[0mClosest candidates are:\n\u001b[0m  glmnet!(::Matrix{Float64}, \u001b[91m::Matrix{Float64}\u001b[39m, ::Binomial; offsets, weights, alpha, penalty_factor, constraints, dfmax, pmax, nlambda, lambda_min_ratio, lambda, tol, standardize, intercept, maxit, algorithm) at C:\\Users\\Work\\.julia\\packages\\GLMNet\\C8WKF\\src\\GLMNet.jl:337\n\u001b[0m  glmnet!(::Matrix{Float64}, ::Vector{Float64}) at C:\\Users\\Work\\.julia\\packages\\GLMNet\\C8WKF\\src\\GLMNet.jl:271\u001b[91m got unsupported keyword arguments \"weights\", \"offsets\"\u001b[39m\n\u001b[0m  glmnet!(::Matrix{Float64}, ::Vector{Float64}, \u001b[91m::Normal\u001b[39m; weights, naivealgorithm, alpha, penalty_factor, constraints, dfmax, pmax, nlambda, lambda_min_ratio, lambda, tol, standardize, intercept, maxit) at C:\\Users\\Work\\.julia\\packages\\GLMNet\\C8WKF\\src\\GLMNet.jl:271\u001b[91m got unsupported keyword argument \"offsets\"\u001b[39m\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching glmnet!(::Matrix{Float64}, ::Vector{Float64}, ::Binomial{Float64}; weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], offsets=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\u001b[0mClosest candidates are:\n\u001b[0m  glmnet!(::Matrix{Float64}, \u001b[91m::Matrix{Float64}\u001b[39m, ::Binomial; offsets, weights, alpha, penalty_factor, constraints, dfmax, pmax, nlambda, lambda_min_ratio, lambda, tol, standardize, intercept, maxit, algorithm) at C:\\Users\\Work\\.julia\\packages\\GLMNet\\C8WKF\\src\\GLMNet.jl:337\n\u001b[0m  glmnet!(::Matrix{Float64}, ::Vector{Float64}) at C:\\Users\\Work\\.julia\\packages\\GLMNet\\C8WKF\\src\\GLMNet.jl:271\u001b[91m got unsupported keyword arguments \"weights\", \"offsets\"\u001b[39m\n\u001b[0m  glmnet!(::Matrix{Float64}, ::Vector{Float64}, \u001b[91m::Normal\u001b[39m; weights, naivealgorithm, alpha, penalty_factor, constraints, dfmax, pmax, nlambda, lambda_min_ratio, lambda, tol, standardize, intercept, maxit) at C:\\Users\\Work\\.julia\\packages\\GLMNet\\C8WKF\\src\\GLMNet.jl:271\u001b[91m got unsupported keyword argument \"offsets\"\u001b[39m\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      " [1] glmnet(X::Matrix{Float64}, y::Vector{Float64}, family::Binomial{Float64}; kw::Base.Pairs{Symbol, Vector{Float64}, Tuple{Symbol, Symbol}, NamedTuple{(:weights, :offsets), Tuple{Vector{Float64}, Vector{Float64}}}})",
      "   @ GLMNet C:\\Users\\Work\\.julia\\packages\\GLMNet\\C8WKF\\src\\GLMNet.jl:485",
      " [2] glmnetcv(X::Matrix{Float64}, y::Vector{Int64}, family::Binomial{Float64}; weights::Vector{Float64}, offsets::Nothing, rng::Random._GLOBAL_RNG, nfolds::Int64, folds::Vector{Int64}, parallel::Bool, grouped::Bool, kw::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "   @ GLMNet C:\\Users\\Work\\.julia\\packages\\GLMNet\\C8WKF\\src\\GLMNet.jl:543",
      " [3] ml_r(x::Matrix{Float64}, d::Vector{Int64})",
      "   @ Main .\\In[146]:3",
      " [4] IIVM_Boost(x::Matrix{Float64}, y::Vector{Int32}, d::Vector{Int64}, z::Vector{Int64}, ml_g::typeof(ml_g), ml_r::typeof(ml_r), ml_m::typeof(ml_m), nfold::Int64, trimming_threshold::Float64)",
      "   @ Main .\\In[139]:26",
      " [5] IIVM_Boost(x::Matrix{Float64}, y::Vector{Int32}, d::Vector{Int64}, z::Vector{Int64}, ml_g::Function, ml_r::Function, ml_m::Function, nfold::Int64)",
      "   @ Main .\\In[139]:4",
      " [6] top-level scope",
      "   @ In[146]:7",
      " [7] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "\n",
    "ml_r(x, d) = glmnetcv(x, d, nfolds = 5, Binomial());\n",
    "ml_g(x, y) = glmnetcv(x, y, nfolds = 5);\n",
    "ml_m(x, z) = glmnetcv(x, z, nfolds = 5, Binomial());\n",
    "\n",
    "boost_fit, boost_data, psi_a, psi_b = IIVM_Boost(x, y, d, z, ml_g, ml_r, ml_m, 3)\n",
    "boost_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6f16d506-9f28-4f68-b355-cfbbaa36bca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IIVM_Forest (generic function with 2 methods)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function IIVM_Forest(x, y, d, z, ml_g, ml_r, ml_m, nfold, trimming_threshold = 1e-12)\n",
    "    \n",
    "    # Sample size\n",
    "    nobser = size(x, 1)\n",
    "    \n",
    "    # Fold indexes\n",
    "    foldid = collect(Kfold(size(x)[1], nfold))\n",
    "    \n",
    "    # Initialize vectors for predictions\n",
    "    y0_hat = ones(nobser)\n",
    "    y1_hat = ones(nobser)\n",
    "    d0_hat = zeros(nobser)\n",
    "    d1_hat = ones(nobser)\n",
    "    z_hat = ones(nobser)\n",
    "    \n",
    "    # Apply learners to y_0, y_1, d_1, d_2 and d separately\n",
    "    for i in 1:nfold\n",
    "        # Create y_0 and y_1 for this fold\n",
    "        mask = findall(==(1), z[foldid[i]])\n",
    "        smp_1 = foldid[i][mask]\n",
    "        smp_0 = foldid[i][Not(mask)]\n",
    "        \n",
    "        # Model Learning\n",
    "        g0_hat = ml_g(x[smp_0, :], y[smp_0])\n",
    "        g1_hat = ml_g(x[smp_1, :], y[smp_1])\n",
    "        r1_hat = ml_r(x[smp_1, :], d[smp_1])\n",
    "        m_hat = ml_m(x[foldid[i], :], z[foldid[i]])\n",
    "        \n",
    "        # Predict: g0_hat, g1_hat, m_hat\n",
    "        d1_hat[Not(foldid[i])] = apply_forest(r1_hat, x[Not(foldid[i]), :])\n",
    "        y0_hat[Not(foldid[i])] = apply_forest(g0_hat, x[Not(foldid[i]), :])\n",
    "        y1_hat[Not(foldid[i])] = apply_forest(g1_hat, x[Not(foldid[i]), :])\n",
    "        z_hat[Not(foldid[i])] = apply_forest(m_hat, x[Not(foldid[i]), :])\n",
    "    \n",
    "    end\n",
    "    \n",
    "    \n",
    "    # Residuals: u0_hat, u1_hat, w0_hat, and w1_hat; no need for residual in z\n",
    "    u0_hat = y .- y0_hat\n",
    "    u1_hat = y .- y1_hat\n",
    "    w0_hat = d .- d0_hat\n",
    "    w1_hat = d .- d1_hat\n",
    "    \n",
    "    # Compute regression terms:\n",
    "    # Left side: y1_hat - y0_hat + z * (u1_hat) / z_hat - (1 - z) * u0_hat / (1 - z_hat)\n",
    "    psi_b = y1_hat .- y0_hat .+ z .* u1_hat ./ z_hat .- (1 .- z) .* u0_hat ./ (1 .- z_hat)\n",
    "    \n",
    "    # Right side: d1_hat - d0_hat + z * (w1_hat) / z_hat - (1 - z) * w0_hat / (1 - z_hat)\n",
    "    psi_a = d1_hat .- d0_hat .+ z .* w1_hat ./ z_hat .- (1 .- z) .* w0_hat ./ (1 .- z_hat)\n",
    "    \n",
    "    # Regression with fit(LinearModel, ...)\n",
    "    rfit = fit(LinearModel, reshape(psi_a, nobser, 1), psi_b)\n",
    "    \n",
    "    # Generate data matrix for output\n",
    "    u_hat = z .* u1_hat + (1 .- z) .* u0_hat\n",
    "    d_til = z .* w1_hat + (1 .- z) .* w0_hat\n",
    "    data = DataFrame(u_hat = u_hat, d_til = d_til)\n",
    "    \n",
    "    # Function outputs residual data and ATE\n",
    "    return rfit, data, psi_a, psi_b;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5586c55b-9c22-43c7-929b-48aeb0b048a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "──────────────────────────────────────────────────────────\n",
       "    Coef.  Std. Error    t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "──────────────────────────────────────────────────────────\n",
       "x1    0.0         NaN  NaN       NaN        NaN        NaN\n",
       "──────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "\n",
    "ml_r(x, d) = build_forest(d, x);\n",
    "ml_g(x, y) = build_forest(y, x);\n",
    "ml_m(x, z) = build_forest(z, x);\n",
    "\n",
    "boost_fit, boost_data, psi_a, psi_b = IIVM_Forest(x, y, d, z, ml_g, ml_r, ml_m, 3)\n",
    "boost_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9132026b-8c4d-40d4-9e98-7ff3fe5cb204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IIVM_Tree (generic function with 2 methods)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function IIVM_Tree(x, y, d, z, ml_g, ml_r, ml_m, nfold, trimming_threshold = 1e-12)\n",
    "    \n",
    "    # Sample size\n",
    "    nobser = size(x, 1)\n",
    "    \n",
    "    # Fold indexes\n",
    "    foldid = collect(Kfold(size(x)[1], nfold))\n",
    "    \n",
    "    # Initialize vectors for predictions\n",
    "    y0_hat = ones(nobser)\n",
    "    y1_hat = ones(nobser)\n",
    "    d0_hat = zeros(nobser)\n",
    "    d1_hat = ones(nobser)\n",
    "    z_hat = ones(nobser)\n",
    "    \n",
    "    # Apply learners to y_0, y_1, d_1, d_2 and d separately\n",
    "    for i in 1:nfold\n",
    "        # Create y_0 and y_1 for this fold\n",
    "        mask = findall(==(1), z[foldid[i]])\n",
    "        smp_1 = foldid[i][mask]\n",
    "        smp_0 = foldid[i][Not(mask)]\n",
    "        \n",
    "        # Model Learning\n",
    "        g0_hat = ml_g(x[smp_0, :], y[smp_0])\n",
    "        g1_hat = ml_g(x[smp_1, :], y[smp_1])\n",
    "        r1_hat = ml_r(x[smp_1, :], d[smp_1])\n",
    "        m_hat = ml_m(x[foldid[i], :], z[foldid[i]])\n",
    "        \n",
    "        # Predict: g0_hat, g1_hat, m_hat\n",
    "        d1_hat[Not(foldid[i])] = apply_tree(r1_hat, x[Not(foldid[i]), :])\n",
    "        y0_hat[Not(foldid[i])] = apply_tree(g0_hat, x[Not(foldid[i]), :])\n",
    "        y1_hat[Not(foldid[i])] = apply_tree(g1_hat, x[Not(foldid[i]), :])\n",
    "        z_hat[Not(foldid[i])] = apply_tree(m_hat, x[Not(foldid[i]), :])\n",
    "    \n",
    "    end\n",
    "    \n",
    "    \n",
    "    # Residuals: u0_hat, u1_hat, w0_hat, and w1_hat; no need for residual in z\n",
    "    u0_hat = y .- y0_hat\n",
    "    u1_hat = y .- y1_hat\n",
    "    w0_hat = d .- d0_hat\n",
    "    w1_hat = d .- d1_hat\n",
    "    \n",
    "    # Compute regression terms:\n",
    "    # Left side: y1_hat - y0_hat + z * (u1_hat) / z_hat - (1 - z) * u0_hat / (1 - z_hat)\n",
    "    psi_b = y1_hat .- y0_hat .+ z .* u1_hat ./ z_hat .- (1 .- z) .* u0_hat ./ (1 .- z_hat)\n",
    "    \n",
    "    # Right side: d1_hat - d0_hat + z * (w1_hat) / z_hat - (1 - z) * w0_hat / (1 - z_hat)\n",
    "    psi_a = d1_hat .- d0_hat .+ z .* w1_hat ./ z_hat .- (1 .- z) .* w0_hat ./ (1 .- z_hat)\n",
    "    \n",
    "    # Regression with fit(LinearModel, ...)\n",
    "    rfit = fit(LinearModel, reshape(psi_a, nobser, 1), psi_b)\n",
    "    \n",
    "    # Generate data matrix for output\n",
    "    u_hat = z .* u1_hat + (1 .- z) .* u0_hat\n",
    "    d_til = z .* w1_hat + (1 .- z) .* w0_hat\n",
    "    data = DataFrame(u_hat = u_hat, d_til = d_til)\n",
    "    \n",
    "    # Function outputs residual data and ATE\n",
    "    return rfit, data, psi_a, psi_b;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fb46cb82-a730-40ef-ab03-06a539ca4427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "──────────────────────────────────────────────────────────\n",
       "    Coef.  Std. Error    t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "──────────────────────────────────────────────────────────\n",
       "x1    0.0         NaN  NaN       NaN        NaN        NaN\n",
       "──────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "\n",
    "ml_r(x, d) = build_tree(d, x, 0, 30, 7, 20, 0.01);\n",
    "ml_g(x, y) = build_tree(y, x, 0, 30, 7, 20, 0.01);\n",
    "ml_m(x, z) = build_tree(z, x, 0, 30, 7, 20, 0.01);\n",
    "\n",
    "boost_fit, boost_data, psi_a, psi_b = IIVM_Tree(x, y, d, z, ml_g, ml_r, ml_m, 3)\n",
    "boost_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d2ffa79b-528e-4b3d-a100-db503e1199e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915-element Vector{Float64}:\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       "  Inf\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       "   ⋮\n",
       "  Inf\n",
       "  Inf\n",
       " -Inf\n",
       "  Inf\n",
       "  Inf\n",
       " -Inf\n",
       "  Inf\n",
       " NaN\n",
       " NaN\n",
       " NaN\n",
       " -Inf\n",
       " -Inf"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2a538-536c-4b4d-a8e8-db226bf9f49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1b0d1257-2770-4b79-9509-1011ed6bc197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IIVM_Boost (generic function with 2 methods)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function IIVM_Boost(x, y, d, z, ml_g, ml_r, ml_m, nfold, trimming_threshold = 1e-12)\n",
    "    \n",
    "    # Sample size\n",
    "    nobser = size(x, 1)\n",
    "    \n",
    "    # Fold indexes\n",
    "    foldid = collect(Kfold(size(x)[1], nfold))\n",
    "    \n",
    "    # Initialize vectors for predictions\n",
    "    y0_hat = ones(nobser)\n",
    "    y1_hat = ones(nobser)\n",
    "    d0_hat = zeros(nobser)\n",
    "    d1_hat = ones(nobser)\n",
    "    z_hat = ones(nobser)\n",
    "    \n",
    "    # Apply learners to y_0, y_1, d_1, d_2 and d separately\n",
    "    for i in 1:nfold\n",
    "        # Create y_0 and y_1 for this fold\n",
    "        mask = findall(==(1), z[foldid[i]])\n",
    "        smp_1 = foldid[i][mask]\n",
    "        smp_0 = foldid[i][Not(mask)]\n",
    "        \n",
    "        # Model Learning\n",
    "        g0_hat = ml_g(x[smp_0, :], y[smp_0])\n",
    "        g1_hat = ml_g(x[smp_1, :], y[smp_1])\n",
    "        r1_hat = ml_r(x[smp_1, :], d[smp_1])\n",
    "        m_hat = ml_m(x[foldid[i], :], z[foldid[i]])\n",
    "        \n",
    "        # Predict: g0_hat, g1_hat, m_hat\n",
    "        d1_hat[Not(foldid[i])] = XGBoost.predict(r1_hat, x[Not(foldid[i]), :])\n",
    "        y0_hat[Not(foldid[i])] = XGBoost.predict(g0_hat, x[Not(foldid[i]), :])\n",
    "        y1_hat[Not(foldid[i])] = XGBoost.predict(g1_hat, x[Not(foldid[i]), :])\n",
    "        z_hat[Not(foldid[i])] = XGBoost.predict(m_hat, x[Not(foldid[i]), :])\n",
    "    \n",
    "    end\n",
    "    \n",
    "    \n",
    "    # Residuals: u0_hat, u1_hat, w0_hat, and w1_hat; no need for residual in z\n",
    "    u0_hat = y .- y0_hat\n",
    "    u1_hat = y .- y1_hat\n",
    "    w0_hat = d .- d0_hat\n",
    "    w1_hat = d .- d1_hat\n",
    "    \n",
    "    # Compute regression terms:\n",
    "    # Left side: y1_hat - y0_hat + z * (u1_hat) / z_hat - (1 - z) * u0_hat / (1 - z_hat)\n",
    "    psi_b = y1_hat .- y0_hat .+ z .* u1_hat ./ z_hat .- (1 .- z) .* u0_hat ./ (1 .- z_hat)\n",
    "    \n",
    "    # Right side: d1_hat - d0_hat + z * (w1_hat) / z_hat - (1 - z) * w0_hat / (1 - z_hat)\n",
    "    psi_a = d1_hat .- d0_hat .+ z .* w1_hat ./ z_hat .- (1 .- z) .* w0_hat ./ (1 .- z_hat)\n",
    "    \n",
    "    # Regression with fit(LinearModel, ...)\n",
    "    rfit = fit(LinearModel, reshape(psi_a, nobser, 1), psi_b)\n",
    "    \n",
    "    # Generate data matrix for output\n",
    "    u_hat = z .* u1_hat + (1 .- z) .* u0_hat\n",
    "    d_til = z .* w1_hat + (1 .- z) .* w0_hat\n",
    "    data = DataFrame(u_hat = u_hat, d_til = d_til)\n",
    "    \n",
    "    # Function outputs residual data and ATE\n",
    "    return rfit, data, psi_a, psi_b;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ed8d33eb-8ac0-4084-9b97-c9316bfe4b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:38737.01316460889211157\n",
      "[2]\ttrain-rmse:33304.54192929357668618\n",
      "[3]\ttrain-rmse:30087.88246757425804390\n",
      "[4]\ttrain-rmse:27682.53218891132928547\n",
      "[5]\ttrain-rmse:25969.88210150778104435\n",
      "[1]\ttrain-rmse:72572.35164468456059694\n",
      "[2]\ttrain-rmse:60784.81608743120159488\n",
      "[3]\ttrain-rmse:52507.22102924608770991\n",
      "[4]\ttrain-rmse:45813.72014589509490179\n",
      "[5]\ttrain-rmse:41249.85934676897886675\n",
      "[1]\ttrain-logloss:0.43798348307609558\n",
      "[2]\ttrain-logloss:0.29688626527786255\n",
      "[3]\ttrain-logloss:0.20793505012989044\n",
      "[4]\ttrain-logloss:0.14840467274188995\n",
      "[5]\ttrain-logloss:0.10719931125640869\n",
      "[1]\ttrain-logloss:0.60419132046728374\n",
      "[2]\ttrain-logloss:0.55244129131042286\n",
      "[3]\ttrain-logloss:0.52205823458584644\n",
      "[4]\ttrain-logloss:0.50040258928209136\n",
      "[5]\ttrain-logloss:0.48396389952180247\n",
      "[1]\ttrain-rmse:52097.06435315485578030\n",
      "[2]\ttrain-rmse:45438.81816587039793376\n",
      "[3]\ttrain-rmse:40529.84053594780561980\n",
      "[4]\ttrain-rmse:36828.59269390138797462\n",
      "[5]\ttrain-rmse:34190.20507247609930346\n",
      "[1]\ttrain-rmse:76699.46711005464021582\n",
      "[2]\ttrain-rmse:66159.79872236897062976\n",
      "[3]\ttrain-rmse:58604.30054447607108159\n",
      "[4]\ttrain-rmse:53187.39922854965698207\n",
      "[5]\ttrain-rmse:48078.70394358545308933\n",
      "[1]\ttrain-logloss:0.43798089027404785\n",
      "[2]\ttrain-logloss:0.29688304662704468\n",
      "[3]\ttrain-logloss:0.20793174207210541\n",
      "[4]\ttrain-logloss:0.14840149879455566\n",
      "[5]\ttrain-logloss:0.10719606280326843\n",
      "[1]\ttrain-logloss:0.60079734626137360\n",
      "[2]\ttrain-logloss:0.54891287613654460\n",
      "[3]\ttrain-logloss:0.51822749015152370\n",
      "[4]\ttrain-logloss:0.49740344255080560\n",
      "[5]\ttrain-logloss:0.48021829179664244\n",
      "[1]\ttrain-rmse:51520.86249363672686741\n",
      "[2]\ttrain-rmse:44869.64527681432809914\n",
      "[3]\ttrain-rmse:40060.30865430940320948\n",
      "[4]\ttrain-rmse:36389.44057544340466848\n",
      "[5]\ttrain-rmse:33519.42267165944940643\n",
      "[1]\ttrain-rmse:70127.85160157522477675\n",
      "[2]\ttrain-rmse:60656.04893145235837437\n",
      "[3]\ttrain-rmse:54494.32637024657014990\n",
      "[4]\ttrain-rmse:48895.69543426703603473\n",
      "[5]\ttrain-rmse:45360.65633690952381585\n",
      "[1]\ttrain-logloss:0.43797209858894348\n",
      "[2]\ttrain-logloss:0.29687252640724182\n",
      "[3]\ttrain-logloss:0.20792074501514435\n",
      "[4]\ttrain-logloss:0.14839050173759460\n",
      "[5]\ttrain-logloss:0.10718551278114319\n",
      "[1]\ttrain-logloss:0.60325615585932391\n",
      "[2]\ttrain-logloss:0.55267446944645060\n",
      "[3]\ttrain-logloss:0.52069095989489156\n",
      "[4]\ttrain-logloss:0.49739827184110835\n",
      "[5]\ttrain-logloss:0.48003386232477813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "─────────────────────────────────────────────────────────────\n",
       "      Coef.  Std. Error     t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "─────────────────────────────────────────────────────────────\n",
       "x1  12176.7     1268.88  9.60    <1e-20    9689.49    14664.0\n",
       "─────────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "\n",
    "ml_r(x, d) = xgboost(x, 5, label = d, objective = \"binary:logistic\", eval_metric = \"logloss\");\n",
    "ml_g(x, y) = xgboost(x, 5, label = y);\n",
    "ml_m(x, z) = xgboost(x, 5, label = z, objective = \"binary:logistic\", eval_metric = \"logloss\");\n",
    "\n",
    "boost_fit, boost_data, psi_a, psi_b = IIVM_Boost(x, y, d, z, ml_g, ml_r, ml_m, 3)\n",
    "boost_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2f249197-4f15-484d-9d67-427e96afe864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier\n",
       "max_depth:                2\n",
       "min_samples_leaf:         1\n",
       "min_samples_split:        2\n",
       "min_purity_increase:      0.0\n",
       "pruning_purity_threshold: 1.0\n",
       "n_subfeatures:            0\n",
       "classes:                  [0, 1]\n",
       "root:                     Decision Tree\n",
       "Leaves: 4\n",
       "Depth:  2"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=2)\n",
    "DecisionTree.fit!(model, x, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d59dcfd6-521c-4d05-a4ef-79d99b7eae34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915-element Vector{Float64}:\n",
       " 0.15991116046640755\n",
       " 0.43381136238279094\n",
       " 0.43381136238279094\n",
       " 0.43381136238279094\n",
       " 0.6444823663253697\n",
       " 0.6444823663253697\n",
       " 0.6444823663253697\n",
       " 0.15991116046640755\n",
       " 0.15991116046640755\n",
       " 0.15991116046640755\n",
       " 0.6444823663253697\n",
       " 0.43381136238279094\n",
       " 0.43381136238279094\n",
       " ⋮\n",
       " 0.43381136238279094\n",
       " 0.43381136238279094\n",
       " 0.43381136238279094\n",
       " 0.15991116046640755\n",
       " 0.15991116046640755\n",
       " 0.43381136238279094\n",
       " 0.43381136238279094\n",
       " 0.43057050592034446\n",
       " 0.6444823663253697\n",
       " 0.43381136238279094\n",
       " 0.6444823663253697\n",
       " 0.15991116046640755"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_proba(model, x)[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "29ff6aed-53cb-457c-ab01-e5fac75c18b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915-element Vector{Int64}:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTree.predict(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5b393d22-b817-4270-a183-4aa75c23780f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 7 < 29000.0 ?\n",
      "├─ Feature 23 < 0.5 ?\n",
      "    ├─ 0 : 3026/3602\n",
      "    └─ 0 : 529/929\n",
      "└─ Feature 23 < 0.5 ?\n",
      "    ├─ 0 : 2053/3626\n",
      "    └─ 1 : 1133/1758\n"
     ]
    }
   ],
   "source": [
    "print_tree(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
