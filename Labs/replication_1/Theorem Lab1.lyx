#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Frisch-Waugh-Lovell Theorem
\end_layout

\begin_layout Section*
Definition
\end_layout

\begin_layout Paragraph

\series medium
The Frisch-Waugh theorem says that the multiple regression coefficient of
 any single variable can also be obtained by first netting out the effect
 of other variable(s) in the regression model from both the dependent variable
 and the independent variable.
 When we consider a linear regression model, we are interested in understanding
 to what extent the independent variables 
\begin_inset Formula $X_{A}$
\end_inset

 and 
\begin_inset Formula $X_{B}$
\end_inset

explain the dispersion (or variance) observed in the dependent variable
 Y .
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Venn Diagram
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename D:/C2021-2/ESTADÍSTICA APLICADA/Captura.JPG
	BoundingBox 444bp 355bp 444bp 355bp
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note: V(x) is the variance
\end_layout

\begin_layout Paragraph*

\series medium
In Figure 1 we can observe a Venn diagram that will facilitate the interpretatio
n of the Frisch-Waugh-Lovell Theorem.
 First, by regressing Y as a function of 
\begin_inset Formula $X_{A}$
\end_inset

 and 
\begin_inset Formula $X_{B}$
\end_inset

, the effect of region A will be captured by 
\begin_inset Formula $\widehat{\beta}_{A}$
\end_inset

 and that of region B will be captured by 
\begin_inset Formula $\widehat{\beta}_{B}$
\end_inset

.
 For its part, area D represents a conflicting region, where we do not know
 to which variable to attribute the effect, since there is collinearity
 (correlation) between 
\begin_inset Formula $X_{A}$
\end_inset

 and 
\begin_inset Formula $X_{B}$
\end_inset

.
 This conflict is known as the "identification problem".
 Suppose we now only regress Y on 
\begin_inset Formula $X_{B}$
\end_inset

.
 The ordinary least squares estimation will attribute to 
\begin_inset Formula $\beta_{B}$
\end_inset

 the effect of region D, which is misleading because part of that effect
 may be due to 
\begin_inset Formula $X_{A}$
\end_inset

 and not to 
\begin_inset Formula $X_{B}$
\end_inset

.
 Now we would fall into the "endogeneity problem" (one of the variables
 with which there is an identification problem is not being observed).
 Therefore, the Frisch-Waugh-Lovell theorem allows us to "clean up" the
 effect of 
\begin_inset Formula $X_{A}$
\end_inset

 .
 By applying it, we no longer have to worry about the region D and we can
 estimate 
\begin_inset Formula $\beta_{B}$
\end_inset

 without any problem.
 
\end_layout

\begin_layout Section*
Explication
\end_layout

\begin_layout Standard
The setup of the theorem is the standard linear model in matrix form: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Y=X\beta+\mu
\]

\end_inset


\end_layout

\begin_layout Paragraph*

\series medium
where 
\begin_inset Formula $Y$
\end_inset

 is an n vector of observations of the dependent variable, 
\begin_inset Formula $X$
\end_inset

 is a 
\begin_inset Formula $n$
\end_inset


\begin_inset Formula $\times k$
\end_inset

 non-stochastic matrix of observations of 
\begin_inset Formula $k$
\end_inset

 explanatory variables, and u is a vector of error terms.
 Let's partition 
\begin_inset Formula $X$
\end_inset

so the model is expressed as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
X=\beta_{1}X_{1}+\beta_{2}X_{2}+\mu
\end{equation}

\end_inset


\end_layout

\begin_layout Paragraph*

\series medium
where 
\begin_inset Formula $X_{1}$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

 are matrices of observations of 
\begin_inset Formula $k_{1}$
\end_inset

and 
\begin_inset Formula $k_{2}$
\end_inset

 explanatory variables, and 
\begin_inset Formula $\beta_{1}$
\end_inset

 and 
\begin_inset Formula $\beta_{2}$
\end_inset

 are the corresponding coeficients vectors.
 Consequently, 
\begin_inset Formula $X=[X_{1}$
\end_inset


\begin_inset Formula $X_{2}$
\end_inset

] , 
\begin_inset Formula $\beta'=[\beta'_{1}$
\end_inset

 
\begin_inset Formula $\beta'_{2}$
\end_inset

] and 
\begin_inset Formula $k=k_{1}+k_{2}$
\end_inset

.
\end_layout

\begin_layout Paragraph*

\series medium
Let 
\begin_inset Formula $M_{1}=I-X_{1}(X'_{1}X_{1})^{-1}X'_{1}$
\end_inset

, that is, 
\begin_inset Formula $M_{1}$
\end_inset

 is an orthogonal projection matrix that projects any vector in 
\begin_inset Formula $R^{n}$
\end_inset

onto the orthogonal complement of the linear space spanned by the columns
 of 
\begin_inset Formula $X_{1}$
\end_inset

.
 Let 
\begin_inset Formula $Y^{*}=M_{1}Y$
\end_inset

 and 
\begin_inset Formula $X_{2}^{*}=M_{1}X_{2}$
\end_inset

.
 
\begin_inset Formula $Y^{*}$
\end_inset

 and 
\begin_inset Formula $X^{*}$
\end_inset

 are, , respectively, OLS residuals of regressing 
\begin_inset Formula $Y$
\end_inset

 and all the columns of 
\family roman
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $X_{2}$
\end_inset

 on 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $X_{1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Supose that we are insterested in estimating (like in previous section)
 in (1), and consider the fallowing alternative methods: 
\end_layout

\begin_layout Itemize
Method 1: Proceed as usual and regress 
\begin_inset Formula $Y$
\end_inset

 on 
\begin_inset Formula $X$
\end_inset

 obtaining the OLS estimator 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\widehat{\beta}=[\hat{\beta}'_{1}$
\end_inset

 
\begin_inset Formula $\hat{\beta}'_{2}]=X_{1}(X'X)^{-1}X'Y$
\end_inset

.
 
\begin_inset Formula $\hat{\beta}{}_{2}$
\end_inset

 would be the desired estimate.
 
\end_layout

\begin_layout Itemize
Method 2 : Regress 
\begin_inset Formula $Y^{*}$
\end_inset

 on 
\begin_inset Formula $X_{2}^{*}$
\end_inset

 and obtain as estimate 
\begin_inset Formula $\tilde{\beta}_{2}=(X_{2}^{*}'X_{1}^{*})^{-1}X_{2}^{*}'Y^{*}$
\end_inset


\end_layout

\begin_layout Paragraph*

\series medium
Let
\series default
 
\begin_inset Formula $e_{1}$
\end_inset

 
\series medium
and
\series default
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $e_{2}$
\end_inset

 be the residuals vectors of the regressions in Method 1 and 2, respectively.
 Now we can state the theorem.
\end_layout

\begin_layout Standard
The theorem says that both methods yield exactly the same estimates of 
\begin_inset Formula $\beta_{2}$
\end_inset

 and that residuals of both regressions are the same.
 That is, an estimate of 
\begin_inset Formula $\beta_{2}$
\end_inset

can be obtained by directly regressing 
\begin_inset Formula $Y$
\end_inset

 on 
\begin_inset Formula $X_{1}$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

or in a two-step fashion.
 In the first step, we 'get rid' of the efect of 
\begin_inset Formula $X_{1}$
\end_inset

 by substracting to 
\begin_inset Formula $Y$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

 the part of them that can be linearly explained by 
\begin_inset Formula $X_{1}$
\end_inset

, and in the second part we run a simple regression using this 'cleaned'
 variables (
\begin_inset Formula $Y^{*}$
\end_inset

 and 
\begin_inset Formula $X^{*}$
\end_inset

).
 
\end_layout

\begin_layout Paragraph*

\series medium
Technically, Method 1 projects 
\begin_inset Formula $Y$
\end_inset

 on the space spanned by the columns of 
\begin_inset Formula $X$
\end_inset

, and its residuals are projections of 
\begin_inset Formula $Y$
\end_inset

 Y on the orthogonal complement of such space.
 Method 2 decomposes this procedure in two steps.
\end_layout

\begin_layout Enumerate
`Eliminates' the effect of 
\begin_inset Formula $X_{1}$
\end_inset

 by first projecting 
\begin_inset Formula $Y$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

 on the orthogonal complement of the space spanned by the columns of 
\begin_inset Formula $X_{1}$
\end_inset

, that is, it creates new variables 
\begin_inset Formula $Y^{*}$
\end_inset

 and 
\begin_inset Formula $X_{2}^{*}$
\end_inset

 which are OLS residuals of regressing 
\begin_inset Formula $Y$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

 on 
\begin_inset Formula $X_{1}$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
Runs OLS on these transformed variables, that is, 
\begin_inset Formula $Y^{*}$
\end_inset

 is projected orthogonally on the space spanned by 
\begin_inset Formula $X_{2}^{*}$
\end_inset

, which, ny construction., is orthogonal to the space spanned by 
\begin_inset Formula $X_{1}$
\end_inset

.
\end_layout

\begin_layout Section*
Algebraic proof 
\end_layout

\begin_layout Paragraph*

\series medium
For completeness, we give a standard algebraic proof of the theorem.
 The starting point is the orthogonal decomposition:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Y=PY+MY=X_{1}\hat{\beta}{}_{1}+X_{2}\hat{\beta}{}_{2}+MY
\]

\end_inset


\end_layout

\begin_layout Paragraph*

\series medium
To prove the first part, multiply both sides by 
\begin_inset Formula $X_{2}^{'}M_{1}$
\end_inset

 and get 
\series default

\begin_inset Formula 
\[
X_{2}^{'}M_{1}Y=X_{2}^{'}M_{1}X_{1}\hat{\beta}{}_{1}+X_{2}^{'}M_{1}X_{2}\hat{\beta}{}_{2}+X_{2}^{'}M_{1}MY
\]

\end_inset


\end_layout

\begin_layout Standard
The first term of the right hand side vanishes since, by definition, 
\begin_inset Formula $M_{1}$
\end_inset

projects 
\begin_inset Formula $X_{1}$
\end_inset

on its orthogonal complement, so 
\begin_inset Formula $M_{1}X_{1}=0$
\end_inset

.
 The third term vanishes too since 
\series medium

\begin_inset Formula $X_{2}^{'}M_{1}M=X_{2}^{'}M-P_{1}X_{2}^{'}M$
\end_inset

 and 
\begin_inset Formula $X_{2}^{'}M=0$
\end_inset

 for the same reasons as before.
 Then, we are left only with the second term.
 Solving for 
\begin_inset Formula $\hat{\beta}{}_{2}$
\end_inset

 proves the first part of the theorem.
 
\end_layout

\begin_layout Paragraph*

\series medium
To prove the second part multiply the orthogonal decomposition by obtain:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M_{1}Y=M_{1}X_{1}\hat{\beta}{}_{1}+M_{1}X_{2}\hat{\beta}{}_{2}+M_{1}MY
\]

\end_inset


\end_layout

\begin_layout Paragraph*

\series medium
Again the first term of the right hand side vanishes.
 Now for the third term, 
\begin_inset Formula $MY$
\end_inset

 belongs to the orthogonal complement of 
\begin_inset Formula $[X_{1}X_{2}]$
\end_inset

, so further projecting it on the orthogonal complement of 
\begin_inset Formula $X_{1}$
\end_inset

(which is what premultiplying by 
\begin_inset Formula $M_{1}$
\end_inset

 would do) has no e®ect, hence 
\begin_inset Formula $M_{1}MY=MY.$
\end_inset

This leaves: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M_{1}Y-M_{1}X_{2}\hat{\beta}{}_{2}=MY
\]

\end_inset


\end_layout

\begin_layout Paragraph*

\series medium
From the first part of the theorem, the left hand side are the errors of
 projecting 
\begin_inset Formula $Y^{*}$
\end_inset

 on 
\begin_inset Formula $X_{2}^{*}$
\end_inset

 and, by definition, the right hand side are the errors of proyecting 
\begin_inset Formula $Y$
\end_inset

 on 
\begin_inset Formula $[X_{1}X_{2}]$
\end_inset

 proving the second part of the theorem.
\end_layout

\end_body
\end_document
