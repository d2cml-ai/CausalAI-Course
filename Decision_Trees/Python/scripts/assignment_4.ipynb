{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Predicting Heart Disease Using Decision Trees and Causal Forest\n",
    "\n",
    "This notebook implements:\n",
    "1. Classification tree for heart disease prediction\n",
    "2. Causal forest analysis for treatment effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Predicting Heart Disease Using a Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Cleaning (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "column_names = ['age', 'sex', 'cp', 'restbp', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'hd']\n",
    "df = pd.read_csv('../input/processed.cleveland.data', names=column_names, na_values='?')\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "# Remove missing values\n",
    "df = df.dropna()\n",
    "print(f\"\\nDataset shape after removing missing values: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variable y (1 if heart disease, 0 otherwise)\n",
    "# Original hd: 0 = no disease, 1-4 = disease levels\n",
    "df['y'] = (df['hd'] > 0).astype(int)\n",
    "\n",
    "print(f\"Distribution of heart disease:\")\n",
    "print(df['y'].value_counts())\n",
    "print(f\"\\nPercentage with heart disease: {df['y'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical variables\n",
    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "# Create dummy variables for categorical features\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_vars, drop_first=True)\n",
    "\n",
    "print(f\"Dataset shape after creating dummy variables: {df_encoded.shape}\")\n",
    "print(f\"\\nColumn names: {list(df_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_encoded.drop(['hd', 'y'], axis=1)\n",
    "y = df_encoded['y']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Analysis (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1 point) Split data and plot classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train initial classification tree (without pruning)\n",
    "clf = DecisionTreeClassifier(random_state=123)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(clf, filled=True, feature_names=X.columns, class_names=[\"No HD\", \"Has HD\"], fontsize=10)\n",
    "plt.title(\"Initial Classification Tree (Unpruned)\")\n",
    "plt.savefig('../output/initial_tree.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Tree depth: {clf.get_depth()}\")\n",
    "print(f\"Number of leaves: {clf.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2 points) Plot confusion matrix and interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Does not have HD\", \"Has HD\"])\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title(f'Confusion Matrix - Initial Tree\\nAccuracy: {accuracy:.4f}')\n",
    "plt.savefig('../output/confusion_matrix_initial.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives: {cm[0, 0]}\")\n",
    "print(f\"False Positives: {cm[0, 1]}\")\n",
    "print(f\"False Negatives: {cm[1, 0]}\")\n",
    "print(f\"True Positives: {cm[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Initial Confusion Matrix:**\n",
    "- The initial tree may show overfitting characteristics with high training accuracy but lower test accuracy\n",
    "- True Positives: Correctly identified patients with heart disease\n",
    "- True Negatives: Correctly identified patients without heart disease\n",
    "- False Positives: Patients incorrectly classified as having heart disease (Type I error)\n",
    "- False Negatives: Patients incorrectly classified as not having heart disease (Type II error - more concerning in medical diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.5 points) Fix overfitting using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 50 alpha values equally spaced on a logarithmic scale between e^-10 and 0.05\n",
    "alphas = np.logspace(-10, np.log10(0.05), 50)\n",
    "print(f\"Alpha range: {alphas.min():.10f} to {alphas.max():.4f}\")\n",
    "print(f\"Number of alphas: {len(alphas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 4-fold cross-validation to select optimal alpha\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=4, shuffle=True, random_state=123)\n",
    "cv_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    clf_cv = DecisionTreeClassifier(ccp_alpha=alpha, random_state=123)\n",
    "    scores = cross_val_score(clf_cv, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "cv_scores = np.array(cv_scores)\n",
    "optimal_idx = np.argmax(cv_scores)\n",
    "optimal_alpha = alphas[optimal_idx]\n",
    "optimal_cv_score = cv_scores[optimal_idx]\n",
    "\n",
    "print(f\"Optimal alpha: {optimal_alpha:.10f}\")\n",
    "print(f\"Best CV accuracy: {optimal_cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.5 points) Plot Inaccuracy Rate (1 - Accuracy) against alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inaccuracy rate\n",
    "inaccuracy_rates = 1 - cv_scores\n",
    "\n",
    "# Plot inaccuracy rate vs alpha\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(alphas, inaccuracy_rates, marker='o', markersize=3, linewidth=2)\n",
    "plt.axvline(optimal_alpha, color='r', linestyle='--', label=f'Optimal α = {optimal_alpha:.6f}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (log scale)', fontsize=12)\n",
    "plt.ylabel('Inaccuracy Rate (1 - Accuracy)', fontsize=12)\n",
    "plt.title('Inaccuracy Rate vs Alpha (4-fold Cross-Validation)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10)\n",
    "plt.savefig('../output/inaccuracy_vs_alpha.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Minimum inaccuracy rate: {inaccuracy_rates.min():.4f}\")\n",
    "print(f\"Maximum inaccuracy rate: {inaccuracy_rates.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2 points) Plot optimal tree and confusion matrix with interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tree with optimal alpha\n",
    "clf_optimal = DecisionTreeClassifier(ccp_alpha=optimal_alpha, random_state=123)\n",
    "clf_optimal.fit(X_train, y_train)\n",
    "\n",
    "# Plot optimal tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(clf_optimal, filled=True, feature_names=X.columns, class_names=[\"No HD\", \"Has HD\"], fontsize=10)\n",
    "plt.title(f\"Optimal Classification Tree (α = {optimal_alpha:.6f})\")\n",
    "plt.savefig('../output/optimal_tree.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal tree depth: {clf_optimal.get_depth()}\")\n",
    "print(f\"Optimal number of leaves: {clf_optimal.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with optimal tree\n",
    "y_pred_optimal = clf_optimal.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
    "accuracy_optimal = accuracy_score(y_test, y_pred_optimal)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_optimal, display_labels=[\"Does not have HD\", \"Has HD\"])\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title(f'Confusion Matrix - Optimal Tree (α = {optimal_alpha:.6f})\\nAccuracy: {accuracy_optimal:.4f}')\n",
    "plt.savefig('../output/confusion_matrix_optimal.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal Test Accuracy: {accuracy_optimal:.4f}\")\n",
    "print(f\"Initial Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\nConfusion Matrix (Optimal):\")\n",
    "print(cm_optimal)\n",
    "print(f\"\\nTrue Negatives: {cm_optimal[0, 0]}\")\n",
    "print(f\"False Positives: {cm_optimal[0, 1]}\")\n",
    "print(f\"False Negatives: {cm_optimal[1, 0]}\")\n",
    "print(f\"True Positives: {cm_optimal[1, 1]}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "sensitivity = cm_optimal[1, 1] / (cm_optimal[1, 1] + cm_optimal[1, 0])\n",
    "specificity = cm_optimal[0, 0] / (cm_optimal[0, 0] + cm_optimal[0, 1])\n",
    "print(f\"\\nSensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
    "print(f\"Specificity (True Negative Rate): {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation and Discussion:**\n",
    "\n",
    "1. **Tree Complexity**: The optimal tree with regularization (α) is simpler than the initial unpruned tree, reducing overfitting.\n",
    "\n",
    "2. **Performance Comparison**: \n",
    "   - The pruned tree may have slightly lower training accuracy but better generalization on test data\n",
    "   - The cross-validation process helped select an alpha that balances bias and variance\n",
    "\n",
    "3. **Clinical Implications**:\n",
    "   - Sensitivity measures the ability to correctly identify patients with heart disease\n",
    "   - Specificity measures the ability to correctly identify patients without heart disease\n",
    "   - In medical diagnosis, high sensitivity is often preferred to avoid missing cases (minimize false negatives)\n",
    "\n",
    "4. **Model Insights**: \n",
    "   - The tree reveals which features are most important for heart disease prediction\n",
    "   - The pruning process removed splits that didn't significantly improve accuracy\n",
    "   - The optimal model provides interpretable decision rules for clinical use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Causal Forest Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.5 points) Create binary treatment variable T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset random seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# Create binary treatment variable (random assignment)\n",
    "df['T'] = np.random.binomial(1, 0.5, size=len(df))\n",
    "\n",
    "print(f\"Treatment distribution:\")\n",
    "print(df['T'].value_counts())\n",
    "print(f\"\\nProportion treated: {df['T'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 point) Create outcome variable Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate outcome variable Y based on the specified formula\n",
    "# Y = (1 + 0.05*age + 0.3*sex + 0.2*restbp) * T + 0.5*oldpeak + ε\n",
    "# where ε ~ N(0, 1)\n",
    "\n",
    "epsilon = np.random.normal(0, 1, size=len(df))\n",
    "df['Y'] = (1 + 0.05 * df['age'] + 0.3 * df['sex'] + 0.2 * df['restbp']) * df['T'] + 0.5 * df['oldpeak'] + epsilon\n",
    "\n",
    "print(f\"Outcome variable Y statistics:\")\n",
    "print(df['Y'].describe())\n",
    "print(f\"\\nMean Y for treated: {df[df['T']==1]['Y'].mean():.4f}\")\n",
    "print(f\"Mean Y for control: {df[df['T']==0]['Y'].mean():.4f}\")\n",
    "print(f\"Raw difference: {df[df['T']==1]['Y'].mean() - df[df['T']==0]['Y'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 point) Calculate treatment effect using OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS regression: Y ~ T\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.stats as stats\n",
    "\n",
    "X_ols = df[['T']].values\n",
    "y_ols = df['Y'].values\n",
    "\n",
    "ols_model = LinearRegression()\n",
    "ols_model.fit(X_ols, y_ols)\n",
    "\n",
    "treatment_effect_ols = ols_model.coef_[0]\n",
    "intercept = ols_model.intercept_\n",
    "\n",
    "# Calculate standard errors and p-values\n",
    "predictions = ols_model.predict(X_ols)\n",
    "residuals = y_ols - predictions\n",
    "n = len(y_ols)\n",
    "k = 1  # number of predictors\n",
    "dof = n - k - 1\n",
    "mse = np.sum(residuals**2) / dof\n",
    "\n",
    "# Standard error\n",
    "var_T = np.var(X_ols, ddof=1)\n",
    "se_coef = np.sqrt(mse / (n * var_T))\n",
    "t_stat = treatment_effect_ols / se_coef\n",
    "p_value = 2 * (1 - stats.t.cdf(np.abs(t_stat), dof))\n",
    "\n",
    "print(\"OLS Regression Results: Y ~ T\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Intercept: {intercept:.4f}\")\n",
    "print(f\"Treatment Effect (β_T): {treatment_effect_ols:.4f}\")\n",
    "print(f\"Standard Error: {se_coef:.4f}\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.6f}\")\n",
    "print(f\"R-squared: {ols_model.score(X_ols, y_ols):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 points) Use Random Forest to estimate causal effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for causal forest (excluding treatment and outcome)\n",
    "covariates = ['age', 'sex', 'cp', 'restbp', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
    "X_cf = df[covariates].copy()\n",
    "T_cf = df['T'].values\n",
    "Y_cf = df['Y'].values\n",
    "\n",
    "# Since we're using sklearn's Random Forest, we'll estimate heterogeneous treatment effects\n",
    "# by creating interaction features with treatment\n",
    "X_cf_with_treatment = X_cf.copy()\n",
    "X_cf_with_treatment['T'] = T_cf\n",
    "\n",
    "# Add interaction terms (T * covariates)\n",
    "for col in covariates:\n",
    "    X_cf_with_treatment[f'T_x_{col}'] = X_cf_with_treatment['T'] * X_cf_with_treatment[col]\n",
    "\n",
    "# Fit Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=123, max_depth=10, min_samples_leaf=5)\n",
    "rf_model.fit(X_cf_with_treatment, Y_cf)\n",
    "\n",
    "# Predict outcomes under treatment and control for each individual\n",
    "X_treated = X_cf_with_treatment.copy()\n",
    "X_treated['T'] = 1\n",
    "for col in covariates:\n",
    "    X_treated[f'T_x_{col}'] = X_treated[col]\n",
    "\n",
    "X_control = X_cf_with_treatment.copy()\n",
    "X_control['T'] = 0\n",
    "for col in covariates:\n",
    "    X_control[f'T_x_{col}'] = 0\n",
    "\n",
    "Y_pred_treated = rf_model.predict(X_treated)\n",
    "Y_pred_control = rf_model.predict(X_control)\n",
    "\n",
    "# Individual treatment effects\n",
    "individual_treatment_effects = Y_pred_treated - Y_pred_control\n",
    "df['ITE'] = individual_treatment_effects\n",
    "\n",
    "print(\"Random Forest Causal Effects Estimation\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Average Treatment Effect (ATE): {individual_treatment_effects.mean():.4f}\")\n",
    "print(f\"Standard Deviation of ITE: {individual_treatment_effects.std():.4f}\")\n",
    "print(f\"Min ITE: {individual_treatment_effects.min():.4f}\")\n",
    "print(f\"Max ITE: {individual_treatment_effects.max():.4f}\")\n",
    "print(f\"\\nR-squared: {rf_model.score(X_cf_with_treatment, Y_cf):.4f}\")\n",
    "\n",
    "# Plot distribution of treatment effects\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(individual_treatment_effects, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(individual_treatment_effects.mean(), color='r', linestyle='--', linewidth=2, label=f'Mean ITE = {individual_treatment_effects.mean():.4f}')\n",
    "plt.xlabel('Individual Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Individual Treatment Effects', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../output/ite_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 points) Plot representative tree with max_depth=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single decision tree with max_depth=2 to visualize treatment effect heterogeneity\n",
    "tree_model = DecisionTreeRegressor(max_depth=2, random_state=123, min_samples_leaf=10)\n",
    "tree_model.fit(X_cf_with_treatment, Y_cf)\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree_model, filled=True, feature_names=X_cf_with_treatment.columns, fontsize=10, rounded=True)\n",
    "plt.title(\"Representative Tree (max_depth=2) for Treatment Effect Heterogeneity\", fontsize=14)\n",
    "plt.savefig('../output/representative_tree.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Tree depth: {tree_model.get_depth()}\")\n",
    "print(f\"Number of leaves: {tree_model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Representative Tree:**\n",
    "\n",
    "This shallow tree (max_depth=2) reveals the most important heterogeneous treatment effects:\n",
    "- The tree shows which patient characteristics lead to different treatment effects\n",
    "- Each split represents a key decision point that differentiates treatment response\n",
    "- Leaf nodes show the predicted outcome for patients in that subgroup\n",
    "- The interaction terms (T_x_*) capture how treatment effects vary by patient characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.5 points) Feature importance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_cf_with_treatment.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 15 most important features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Top 15 Feature Importances (Random Forest)', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 points) Covariate distribution by treatment effect terciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize covariates\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standardized = pd.DataFrame(\n",
    "    scaler.fit_transform(X_cf),\n",
    "    columns=covariates,\n",
    "    index=X_cf.index\n",
    ")\n",
    "\n",
    "# Divide predicted treatment effects into terciles\n",
    "df['ITE_tercile'] = pd.qcut(df['ITE'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(\"Treatment Effect Terciles:\")\n",
    "print(df.groupby('ITE_tercile')['ITE'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean of each standardized covariate within each tercile\n",
    "tercile_means = []\n",
    "\n",
    "for tercile in ['Low', 'Medium', 'High']:\n",
    "    mask = df['ITE_tercile'] == tercile\n",
    "    tercile_mean = X_standardized[mask].mean()\n",
    "    tercile_means.append(tercile_mean)\n",
    "\n",
    "# Create DataFrame for heatmap\n",
    "heatmap_data = pd.DataFrame(tercile_means, index=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='RdBu_r', center=0, \n",
    "            cbar_kws={'label': 'Mean Standardized Value'}, linewidths=0.5)\n",
    "plt.xlabel('Covariates', fontsize=12)\n",
    "plt.ylabel('Treatment Effect Tercile', fontsize=12)\n",
    "plt.title('Mean Standardized Covariates by Predicted Treatment Effect Terciles', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/terciles_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMean Standardized Covariates by Tercile:\")\n",
    "print(heatmap_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Tercile Analysis:**\n",
    "\n",
    "This heatmap shows how patient characteristics differ across treatment effect terciles:\n",
    "\n",
    "- **Low tercile**: Patients with lowest predicted treatment effects\n",
    "- **Medium tercile**: Patients with moderate predicted treatment effects  \n",
    "- **High tercile**: Patients with highest predicted treatment effects\n",
    "\n",
    "The color intensity indicates how each covariate's mean differs from zero (the population mean after standardization):\n",
    "- Red colors indicate above-average values for that tercile\n",
    "- Blue colors indicate below-average values for that tercile\n",
    "\n",
    "This analysis helps identify which patient characteristics are associated with higher or lower treatment effects, informing targeted intervention strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "\n",
    "### Part 1: Classification Tree\n",
    "- Successfully built and pruned a classification tree for heart disease prediction\n",
    "- Used cross-validation to find optimal complexity parameter (alpha)\n",
    "- Achieved reasonable accuracy while maintaining interpretability\n",
    "\n",
    "### Part 2: Causal Forest\n",
    "- Estimated heterogeneous treatment effects using Random Forest\n",
    "- Identified key characteristics associated with treatment response\n",
    "- Visualized treatment effect heterogeneity across patient subgroups\n",
    "\n",
    "All figures have been saved to the `../output/` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
